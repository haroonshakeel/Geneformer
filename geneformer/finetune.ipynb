{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e08174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /scratchdata1/users/a1841503/Geneformer/Jupyter\n",
      "New directory: /scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check current directory\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# Change to your code directory (replace with your actual path)\n",
    "os.chdir('/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer')\n",
    "\n",
    "# Verify the change\n",
    "print(\"New directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e0faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change to the repository root\n",
    "os.chdir(\"/scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer\")\n",
    "\n",
    "# Add current directory to Python path\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be50793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer.finetuner import FineTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba456902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer.finetuner_utils import get_train_valid_test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f66a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8bb6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running task=disease_classification, dataset=genecorpus_heart_disease, aggregation_level=metacell_32\n",
      "Cuda available: True\n",
      "Using 4 GPU(s)\n",
      "GENE_MEDIAN_FILE: /hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/Geneformer/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl\n",
      "TOKEN_DICTIONARY_FILE: /hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/Geneformer/geneformer/gene_dictionaries_30m/token_dictionary_gc30M.pkl\n",
      "ENSEMBL_MAPPING_FILE: /hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/Geneformer/geneformer/gene_dictionaries_30m/ensembl_mapping_dict_gc30M.pkl\n",
      "Output directory: /hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_metacell_32\n",
      "Classifier type: cell\n",
      "✓ Using final trained model: /hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_foundation_models/models/30M_AGGmetacell_32_6_emb256_SL2048_E2_B12_LR0.001_LSlinear_WU10000_Oadamw/final_trained_model/metacell_32\n",
      "Final pretrained model path: /hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_foundation_models/models/30M_AGGmetacell_32_6_emb256_SL2048_E2_B12_LR0.001_LSlinear_WU10000_Oadamw/final_trained_model/metacell_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_metacell_32/_geneformer_cellClassifier_test/’: File exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef4649b8ac14ddea466392ca4a478e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_metacell_32/_geneformer_cellClassifier_test/ksplit1’: File exists\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_foundation_models/models/30M_AGGmetacell_32_6_emb256_SL2048_E2_B12_LR0.001_LSlinear_WU10000_Oadamw/final_trained_model/metacell_32 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation split: 1/1 ******\n",
      "\n",
      "Total parameters: 10,263,555\n",
      "Trainable parameters before freezing model: 10,263,555\n",
      "Non-trainable parameters before freezing model: 0\n",
      "Trainable parameters after freezing encoder layers: 7,100,931\n",
      "Non-trainable parameters after freezing encoder layers: 3,162,624\n",
      "Freezing parameter: bert.embeddings.word_embeddings.weight\n",
      "Freezing parameter: bert.embeddings.position_embeddings.weight\n",
      "Freezing parameter: bert.embeddings.token_type_embeddings.weight\n",
      "Freezing parameter: bert.embeddings.LayerNorm.weight\n",
      "Freezing parameter: bert.embeddings.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.0.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.0.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.0.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.0.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.0.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.0.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.1.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.1.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.1.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.1.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.1.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.1.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.2.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.2.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.2.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.2.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.2.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.2.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.3.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.3.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.3.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.3.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.3.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.3.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.4.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.4.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.4.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.4.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.4.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.4.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.query.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.query.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.key.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.key.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.value.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.self.value.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "Freezing parameter: bert.encoder.layer.5.intermediate.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.5.intermediate.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.5.output.dense.weight\n",
      "Freezing parameter: bert.encoder.layer.5.output.dense.bias\n",
      "Freezing parameter: bert.encoder.layer.5.output.LayerNorm.weight\n",
      "Freezing parameter: bert.encoder.layer.5.output.LayerNorm.bias\n",
      "Freezing parameter: bert.pooler.dense.weight\n",
      "Freezing parameter: bert.pooler.dense.bias\n",
      "Keeping trainable: classifier.weight\n",
      "Keeping trainable: classifier.bias\n",
      "Trainable parameters after freezing entire model: 771\n",
      "Non-trainable parameters after freezing entire model: 10,262,784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer/collator_for_classification.py:649: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v.clone().detach(), dtype=torch.int64) if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
      "/hpcfs/users/a1841503/myconda/envs/jupyter_geneformer/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/460 00:21 < 27:00, 0.28 it/s, Epoch 0.15/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 80\u001b[0m\n\u001b[1;32m     72\u001b[0m     finetuner \u001b[38;5;241m=\u001b[39m FineTuner(base_dir\u001b[38;5;241m=\u001b[39mbase_dir,\n\u001b[1;32m     73\u001b[0m         aggregation_level\u001b[38;5;241m=\u001b[39maggregation_level,\n\u001b[1;32m     74\u001b[0m         model_variant\u001b[38;5;241m=\u001b[39mmodel_variant,\n\u001b[1;32m     75\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m     76\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset)\n\u001b[1;32m     78\u001b[0m     output_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m crossval_splits \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mksplit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(crossval_split)\n\u001b[0;32m---> 80\u001b[0m     all_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcell_state_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcell_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_data_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilter_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_data_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_data_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_test_id_split_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_test_id_split_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_valid_id_split_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_valid_id_split_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_crossval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdosage_sensitivity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcrossval_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreeze_num_encoder_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_num_encoder_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreeze_entire_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_entire_model\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     crossval_split_metrics[\u001b[38;5;28mstr\u001b[39m(crossval_split)] \u001b[38;5;241m=\u001b[39m all_metrics\n\u001b[1;32m     95\u001b[0m metrics_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(finetuner\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer/finetuner.py:300\u001b[0m, in \u001b[0;36mFineTuner.finetune_model\u001b[0;34m(self, training_args, cell_state_dict, filter_data_dict, input_data_file, output_prefix, train_test_id_split_dict, train_valid_id_split_dict, num_crossval_splits, freeze_num_encoder_layers, freeze_entire_model)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_crossval_splits must be either 1 or 5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Example 6 layer 30M Geneformer model: https://huggingface.co/ctheodoris/Geneformer/blob/main/gf-6L-30M-i2048/model.safetensors\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m all_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepared_input_data_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_input_data_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_class_dict_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_id_class_dict.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_id_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_valid_id_split_dict\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# to optimize hyperparameters, set n_hyperopt_trials=100 (or alternative desired # of trials)\u001b[39;49;00m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_metrics\n",
      "File \u001b[0;32m/scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer/classifier.py:815\u001b[0m, in \u001b[0;36mClassifier.validate\u001b[0;34m(self, model_directory, prepared_input_data_file, id_class_dict_file, output_directory, output_prefix, split_id_dict, attr_to_split, attr_to_balance, gene_balance, max_trials, pval_threshold, save_eval_output, predict_eval, predict_trainer, n_hyperopt_trials, save_gene_split_datasets, debug_gene_split_datasets)\u001b[0m\n\u001b[1;32m    813\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mselect(train_indices)\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_hyperopt_trials \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 815\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mksplit_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperopt_classifier(\n\u001b[1;32m    825\u001b[0m         model_directory,\n\u001b[1;32m    826\u001b[0m         num_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_hyperopt_trials,\n\u001b[1;32m    831\u001b[0m     )\n",
      "File \u001b[0;32m/scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer/classifier.py:1368\u001b[0m, in \u001b[0;36mClassifier.train_classifier\u001b[0;34m(self, model_directory, num_classes, train_data, eval_data, output_directory, predict)\u001b[0m\n\u001b[1;32m   1358\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m   1359\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1360\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args_init,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcu\u001b[38;5;241m.\u001b[39mcompute_metrics,\n\u001b[1;32m   1365\u001b[0m )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;66;03m# train the classifier\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1369\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_directory)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;66;03m# make eval predictions and save predictions and metrics\u001b[39;00m\n",
      "File \u001b[0;32m/hpcfs/users/a1841503/myconda/envs/jupyter_geneformer/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hpcfs/users/a1841503/myconda/envs/jupyter_geneformer/lib/python3.12/site-packages/transformers/trainer.py:2479\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task_config = {\n",
    "        \"tasks\": {\n",
    "        \"disease_classification\": [\n",
    "            \"genecorpus_heart_disease\", \n",
    "            \"cellnexus_blood_disease\", \n",
    "            \"cellnexus_covid_disease\"\n",
    "            ],\n",
    "        # \"dosage_sensitivity\": [\"genecorpus_dosage_sensitivity\"]\n",
    "        },\n",
    "        \"aggregation_levels\": [\n",
    "            # \"metacell_2\", \n",
    "            # \"metacell_4\", \n",
    "            # \"metacell_8\", \n",
    "            # \"metacell_16\", \n",
    "            \"metacell_32\", \n",
    "            \"metacell_64\", \n",
    "            \"metacell_128\"\n",
    "            ]\n",
    "    }\n",
    "\n",
    "# aggregation_level=\"metacell_32\"\n",
    "# task=\"dosage_sensitivity\"\n",
    "# dataset=\"genecorpus_dosage_sensitivity\"\n",
    "model_version=\"V1\"\n",
    "base_dir= \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer\"\n",
    "model_variant=\"30M\"\n",
    "crossval_splits = 1 # 1 or 5\n",
    "freeze_num_encoder_layers=6\n",
    "freeze_entire_model=True\n",
    "\n",
    "    # Loop over tasks\n",
    "for task, datasets in task_config[\"tasks\"].items():\n",
    "    # Loop over each dataset associated with the task\n",
    "    for dataset in datasets:\n",
    "        # Loop over aggregation levels\n",
    "        for aggregation_level in task_config[\"aggregation_levels\"]:\n",
    "            print(f\"Running task={task}, dataset={dataset}, aggregation_level={aggregation_level}\")\n",
    "\n",
    "            for crossval_split in range(1, crossval_splits + 1):\n",
    "\n",
    "                crossval_split_metrics = {}\n",
    "\n",
    "\n",
    "\n",
    "                training_args = {\n",
    "                \"num_train_epochs\": 10,\n",
    "                \"learning_rate\": 0.000804,\n",
    "                \"lr_scheduler_type\": \"polynomial\",\n",
    "                \"warmup_steps\": 1812,\n",
    "                \"weight_decay\":0.258828,\n",
    "                \"per_device_train_batch_size\": 128,\n",
    "                \"seed\": 73,\n",
    "                \"evaluation_strategy\":\"epoch\",        # Evaluate every epoch\n",
    "                \"save_strategy\":\"epoch\",              # Save checkpoint every epoch\n",
    "                \"metric_for_best_model\":\"eval_loss\",  # Metric to determine \"best\" model # Doc: https://huggingface.co/transformers/v3.5.1/main_classes/trainer.html#:~:text=after%20each%20evaluation.-,metric_for_best_model,-(str%2C\n",
    "                \"greater_is_better\":False,            # For loss, lower is better\n",
    "                \"load_best_model_at_end\":True,        # KEY: Load best model at the end\n",
    "                \"save_total_limit\":3,                 # Keep only 3 best checkpoints\n",
    "                # \"logging_dir\": os.path.normpath(\"D:/geneformer_finetuning/trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_metacell_8/250623_geneformer_cellClassifier_genecorpus_heart_disease_test/ksplit1/runs\"),\n",
    "                }\n",
    "                \n",
    "\n",
    "                input_data_file, cell_state_dict, filter_data_dict, train_test_id_split_dict, train_valid_id_split_dict = get_train_valid_test_splits(\n",
    "                    TASK=task,\n",
    "                    DATASET=dataset,\n",
    "                    MODEL_VARIANT=model_variant,\n",
    "                    DATASET_PATH = os.path.join(base_dir, \"datasets\", task, dataset)\n",
    ",\n",
    "                    CROSSVAL_SPLITS=crossval_splits,\n",
    "                )\n",
    "\n",
    "                finetuner = FineTuner(base_dir=base_dir,\n",
    "                    aggregation_level=aggregation_level,\n",
    "                    model_variant=model_variant,\n",
    "                    task=task,\n",
    "                    dataset=dataset)\n",
    "                \n",
    "                output_prefix = \"test\" if crossval_splits == 1 else \"ksplit\" + str(crossval_split)\n",
    "    \n",
    "                all_metrics = finetuner.finetune_model(\n",
    "                    training_args=training_args, \n",
    "                    cell_state_dict = cell_state_dict, \n",
    "                    filter_data_dict = filter_data_dict, \n",
    "                    input_data_file = input_data_file, \n",
    "                    output_prefix = output_prefix, \n",
    "                    train_test_id_split_dict = train_test_id_split_dict, \n",
    "                    train_valid_id_split_dict = train_valid_id_split_dict, \n",
    "                    num_crossval_splits=1 if task != \"dosage_sensitivity\" else crossval_splits,\n",
    "                    freeze_num_encoder_layers=freeze_num_encoder_layers,\n",
    "                    freeze_entire_model=freeze_entire_model\n",
    "                    )\n",
    "                \n",
    "                crossval_split_metrics[str(crossval_split)] = all_metrics\n",
    "\n",
    "            metrics_path = os.path.join(finetuner.output_dir, \"metrics.json\")\n",
    "\n",
    "            # Save the metrics to that path\n",
    "            with open(metrics_path, \"w\") as f:\n",
    "                json.dump(crossval_split_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4264be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d7ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully!\n",
      "Dataset length: 579159\n",
      "Dataset features: ['input_ids', 'length', 'cell_type', 'individual', 'age', 'sex', 'disease', 'lvef']\n",
      "\n",
      "=== ANALYSIS RESULTS ===\n",
      "\n",
      "Number of unique individuals: 42\n",
      "\n",
      "Unique individual IDs:\n",
      "  1. 1290\n",
      "  2. 1300\n",
      "  3. 1304\n",
      "  4. 1358\n",
      "  5. 1371\n",
      "  6. 1422\n",
      "  7. 1425\n",
      "  8. 1430\n",
      "  9. 1437\n",
      "  10. 1447\n",
      "  11. 1462\n",
      "  12. 1472\n",
      "  13. 1479\n",
      "  14. 1504\n",
      "  15. 1508\n",
      "  16. 1510\n",
      "  17. 1515\n",
      "  18. 1516\n",
      "  19. 1539\n",
      "  20. 1540\n",
      "  21. 1547\n",
      "  22. 1549\n",
      "  23. 1558\n",
      "  24. 1561\n",
      "  25. 1582\n",
      "  26. 1600\n",
      "  27. 1602\n",
      "  28. 1603\n",
      "  29. 1606\n",
      "  30. 1610\n",
      "  31. 1617\n",
      "  32. 1622\n",
      "  33. 1630\n",
      "  34. 1631\n",
      "  35. 1678\n",
      "  36. 1685\n",
      "  37. 1702\n",
      "  38. 1707\n",
      "  39. 1718\n",
      "  40. 1722\n",
      "  41. 1726\n",
      "  42. 1735\n",
      "\n",
      "Overall disease distribution:\n",
      "  hcm: 230652 samples\n",
      "  nf: 182317 samples\n",
      "  dcm: 166190 samples\n",
      "\n",
      "Disease distribution per individual:\n",
      "==================================================\n",
      "\n",
      "Individual ID: 1290\n",
      "Total samples: 6996\n",
      "Disease distribution:\n",
      "  - dcm: 6996 samples (100.0%)\n",
      "\n",
      "Individual ID: 1300\n",
      "Total samples: 16426\n",
      "Disease distribution:\n",
      "  - dcm: 16426 samples (100.0%)\n",
      "\n",
      "Individual ID: 1304\n",
      "Total samples: 18593\n",
      "Disease distribution:\n",
      "  - dcm: 18593 samples (100.0%)\n",
      "\n",
      "Individual ID: 1358\n",
      "Total samples: 23969\n",
      "Disease distribution:\n",
      "  - dcm: 23969 samples (100.0%)\n",
      "\n",
      "Individual ID: 1371\n",
      "Total samples: 16039\n",
      "Disease distribution:\n",
      "  - dcm: 16039 samples (100.0%)\n",
      "\n",
      "Individual ID: 1422\n",
      "Total samples: 23315\n",
      "Disease distribution:\n",
      "  - hcm: 23315 samples (100.0%)\n",
      "\n",
      "Individual ID: 1425\n",
      "Total samples: 14998\n",
      "Disease distribution:\n",
      "  - hcm: 14998 samples (100.0%)\n",
      "\n",
      "Individual ID: 1430\n",
      "Total samples: 10218\n",
      "Disease distribution:\n",
      "  - dcm: 10218 samples (100.0%)\n",
      "\n",
      "Individual ID: 1437\n",
      "Total samples: 21340\n",
      "Disease distribution:\n",
      "  - dcm: 21340 samples (100.0%)\n",
      "\n",
      "Individual ID: 1447\n",
      "Total samples: 10654\n",
      "Disease distribution:\n",
      "  - hcm: 10654 samples (100.0%)\n",
      "\n",
      "Individual ID: 1462\n",
      "Total samples: 21438\n",
      "Disease distribution:\n",
      "  - hcm: 21438 samples (100.0%)\n",
      "\n",
      "Individual ID: 1472\n",
      "Total samples: 19205\n",
      "Disease distribution:\n",
      "  - dcm: 19205 samples (100.0%)\n",
      "\n",
      "Individual ID: 1479\n",
      "Total samples: 18491\n",
      "Disease distribution:\n",
      "  - hcm: 18491 samples (100.0%)\n",
      "\n",
      "Individual ID: 1504\n",
      "Total samples: 8000\n",
      "Disease distribution:\n",
      "  - dcm: 8000 samples (100.0%)\n",
      "\n",
      "Individual ID: 1508\n",
      "Total samples: 20536\n",
      "Disease distribution:\n",
      "  - hcm: 20536 samples (100.0%)\n",
      "\n",
      "Individual ID: 1510\n",
      "Total samples: 10320\n",
      "Disease distribution:\n",
      "  - hcm: 10320 samples (100.0%)\n",
      "\n",
      "Individual ID: 1515\n",
      "Total samples: 14302\n",
      "Disease distribution:\n",
      "  - nf: 14302 samples (100.0%)\n",
      "\n",
      "Individual ID: 1516\n",
      "Total samples: 9361\n",
      "Disease distribution:\n",
      "  - nf: 9361 samples (100.0%)\n",
      "\n",
      "Individual ID: 1539\n",
      "Total samples: 10711\n",
      "Disease distribution:\n",
      "  - nf: 10711 samples (100.0%)\n",
      "\n",
      "Individual ID: 1540\n",
      "Total samples: 11636\n",
      "Disease distribution:\n",
      "  - nf: 11636 samples (100.0%)\n",
      "\n",
      "Individual ID: 1547\n",
      "Total samples: 7232\n",
      "Disease distribution:\n",
      "  - nf: 7232 samples (100.0%)\n",
      "\n",
      "Individual ID: 1549\n",
      "Total samples: 11703\n",
      "Disease distribution:\n",
      "  - nf: 11703 samples (100.0%)\n",
      "\n",
      "Individual ID: 1558\n",
      "Total samples: 10258\n",
      "Disease distribution:\n",
      "  - nf: 10258 samples (100.0%)\n",
      "\n",
      "Individual ID: 1561\n",
      "Total samples: 9563\n",
      "Disease distribution:\n",
      "  - nf: 9563 samples (100.0%)\n",
      "\n",
      "Individual ID: 1582\n",
      "Total samples: 18854\n",
      "Disease distribution:\n",
      "  - nf: 18854 samples (100.0%)\n",
      "\n",
      "Individual ID: 1600\n",
      "Total samples: 14814\n",
      "Disease distribution:\n",
      "  - nf: 14814 samples (100.0%)\n",
      "\n",
      "Individual ID: 1602\n",
      "Total samples: 16580\n",
      "Disease distribution:\n",
      "  - hcm: 16580 samples (100.0%)\n",
      "\n",
      "Individual ID: 1603\n",
      "Total samples: 10638\n",
      "Disease distribution:\n",
      "  - nf: 10638 samples (100.0%)\n",
      "\n",
      "Individual ID: 1606\n",
      "Total samples: 8052\n",
      "Disease distribution:\n",
      "  - dcm: 8052 samples (100.0%)\n",
      "\n",
      "Individual ID: 1610\n",
      "Total samples: 13426\n",
      "Disease distribution:\n",
      "  - nf: 13426 samples (100.0%)\n",
      "\n",
      "Individual ID: 1617\n",
      "Total samples: 17352\n",
      "Disease distribution:\n",
      "  - dcm: 17352 samples (100.0%)\n",
      "\n",
      "Individual ID: 1622\n",
      "Total samples: 7210\n",
      "Disease distribution:\n",
      "  - nf: 7210 samples (100.0%)\n",
      "\n",
      "Individual ID: 1630\n",
      "Total samples: 17569\n",
      "Disease distribution:\n",
      "  - hcm: 17569 samples (100.0%)\n",
      "\n",
      "Individual ID: 1631\n",
      "Total samples: 13354\n",
      "Disease distribution:\n",
      "  - hcm: 13354 samples (100.0%)\n",
      "\n",
      "Individual ID: 1678\n",
      "Total samples: 9984\n",
      "Disease distribution:\n",
      "  - nf: 9984 samples (100.0%)\n",
      "\n",
      "Individual ID: 1685\n",
      "Total samples: 9801\n",
      "Disease distribution:\n",
      "  - hcm: 9801 samples (100.0%)\n",
      "\n",
      "Individual ID: 1702\n",
      "Total samples: 13550\n",
      "Disease distribution:\n",
      "  - nf: 13550 samples (100.0%)\n",
      "\n",
      "Individual ID: 1707\n",
      "Total samples: 8967\n",
      "Disease distribution:\n",
      "  - hcm: 8967 samples (100.0%)\n",
      "\n",
      "Individual ID: 1718\n",
      "Total samples: 9075\n",
      "Disease distribution:\n",
      "  - nf: 9075 samples (100.0%)\n",
      "\n",
      "Individual ID: 1722\n",
      "Total samples: 21261\n",
      "Disease distribution:\n",
      "  - hcm: 21261 samples (100.0%)\n",
      "\n",
      "Individual ID: 1726\n",
      "Total samples: 11721\n",
      "Disease distribution:\n",
      "  - hcm: 11721 samples (100.0%)\n",
      "\n",
      "Individual ID: 1735\n",
      "Total samples: 11647\n",
      "Disease distribution:\n",
      "  - hcm: 11647 samples (100.0%)\n",
      "\n",
      "==================================================\n",
      "SUMMARY STATISTICS\n",
      "==================================================\n",
      "\n",
      "Samples per individual:\n",
      "  Min: 6996\n",
      "  Max: 23969\n",
      "  Mean: 13789.5\n",
      "  Median: 12537.5\n",
      "\n",
      "Cross-tabulation (Individual vs Disease):\n",
      "disease        dcm     hcm      nf     All\n",
      "individual                                \n",
      "1290          6996       0       0    6996\n",
      "1300         16426       0       0   16426\n",
      "1304         18593       0       0   18593\n",
      "1358         23969       0       0   23969\n",
      "1371         16039       0       0   16039\n",
      "1422             0   23315       0   23315\n",
      "1425             0   14998       0   14998\n",
      "1430         10218       0       0   10218\n",
      "1437         21340       0       0   21340\n",
      "1447             0   10654       0   10654\n",
      "1462             0   21438       0   21438\n",
      "1472         19205       0       0   19205\n",
      "1479             0   18491       0   18491\n",
      "1504          8000       0       0    8000\n",
      "1508             0   20536       0   20536\n",
      "1510             0   10320       0   10320\n",
      "1515             0       0   14302   14302\n",
      "1516             0       0    9361    9361\n",
      "1539             0       0   10711   10711\n",
      "1540             0       0   11636   11636\n",
      "1547             0       0    7232    7232\n",
      "1549             0       0   11703   11703\n",
      "1558             0       0   10258   10258\n",
      "1561             0       0    9563    9563\n",
      "1582             0       0   18854   18854\n",
      "1600             0       0   14814   14814\n",
      "1602             0   16580       0   16580\n",
      "1603             0       0   10638   10638\n",
      "1606          8052       0       0    8052\n",
      "1610             0       0   13426   13426\n",
      "1617         17352       0       0   17352\n",
      "1622             0       0    7210    7210\n",
      "1630             0   17569       0   17569\n",
      "1631             0   13354       0   13354\n",
      "1678             0       0    9984    9984\n",
      "1685             0    9801       0    9801\n",
      "1702             0       0   13550   13550\n",
      "1707             0    8967       0    8967\n",
      "1718             0       0    9075    9075\n",
      "1722             0   21261       0   21261\n",
      "1726             0   11721       0   11721\n",
      "1735             0   11647       0   11647\n",
      "All         166190  230652  182317  579159\n",
      "\n",
      "Individuals with multiple disease types:\n",
      "  No individuals have multiple disease types\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Read the HuggingFace dataset\n",
    "dataset_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/human_dcm_hcm_nf.dataset/\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset = Dataset.load_from_disk(dataset_path)\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'individual' not in df.columns or 'disease' not in df.columns:\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        print(\"Please check if 'individual' and 'disease' columns exist with these exact names\")\n",
    "    else:\n",
    "        print(\"=== ANALYSIS RESULTS ===\")\n",
    "        print()\n",
    "        \n",
    "        # 1. Number of unique individuals\n",
    "        unique_individuals = df['individual'].nunique()\n",
    "        print(f\"Number of unique individuals: {unique_individuals}\")\n",
    "        print()\n",
    "        \n",
    "        # 2. List all unique individuals\n",
    "        individual_ids = df['individual'].unique()\n",
    "        print(\"Unique individual IDs:\")\n",
    "        for i, ind_id in enumerate(sorted(individual_ids), 1):\n",
    "            print(f\"  {i}. {ind_id}\")\n",
    "        print()\n",
    "        \n",
    "        # 3. Overall disease distribution\n",
    "        disease_counts = df['disease'].value_counts()\n",
    "        print(\"Overall disease distribution:\")\n",
    "        for disease, count in disease_counts.items():\n",
    "            print(f\"  {disease}: {count} samples\")\n",
    "        print()\n",
    "        \n",
    "        # 4. Disease distribution per individual\n",
    "        print(\"Disease distribution per individual:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for ind_id in sorted(individual_ids):\n",
    "            individual_data = df[df['individual'] == ind_id]\n",
    "            disease_dist = individual_data['disease'].value_counts()\n",
    "            total_samples = len(individual_data)\n",
    "            \n",
    "            print(f\"\\nIndividual ID: {ind_id}\")\n",
    "            print(f\"Total samples: {total_samples}\")\n",
    "            print(\"Disease distribution:\")\n",
    "            for disease, count in disease_dist.items():\n",
    "                percentage = (count / total_samples) * 100\n",
    "                print(f\"  - {disease}: {count} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 5. Summary statistics\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Samples per individual\n",
    "        samples_per_individual = df.groupby('individual').size()\n",
    "        print(f\"\\nSamples per individual:\")\n",
    "        print(f\"  Min: {samples_per_individual.min()}\")\n",
    "        print(f\"  Max: {samples_per_individual.max()}\")\n",
    "        print(f\"  Mean: {samples_per_individual.mean():.1f}\")\n",
    "        print(f\"  Median: {samples_per_individual.median():.1f}\")\n",
    "        \n",
    "        # Cross-tabulation\n",
    "        print(f\"\\nCross-tabulation (Individual vs Disease):\")\n",
    "        crosstab = pd.crosstab(df['individual'], df['disease'], margins=True)\n",
    "        print(crosstab)\n",
    "        \n",
    "        # 6. Check for any individuals with multiple diseases\n",
    "        print(f\"\\nIndividuals with multiple disease types:\")\n",
    "        multi_disease_individuals = df.groupby('individual')['disease'].nunique()\n",
    "        multi_disease = multi_disease_individuals[multi_disease_individuals > 1]\n",
    "        \n",
    "        if len(multi_disease) > 0:\n",
    "            for ind_id, num_diseases in multi_disease.items():\n",
    "                diseases = df[df['individual'] == ind_id]['disease'].unique()\n",
    "                print(f\"  {ind_id}: {num_diseases} diseases - {list(diseases)}\")\n",
    "        else:\n",
    "            print(\"  No individuals have multiple disease types\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {str(e)}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"1. Check if the path exists\")\n",
    "    print(\"2. Verify the dataset format\")\n",
    "    print(\"3. Check column names\")\n",
    "    \n",
    "    # Try to list directory contents\n",
    "    try:\n",
    "        if os.path.exists(dataset_path):\n",
    "            contents = os.listdir(dataset_path)\n",
    "            print(f\"Directory contents: {contents}\")\n",
    "        else:\n",
    "            print(\"Dataset path does not exist\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Cannot access directory: {str(e2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e6f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
