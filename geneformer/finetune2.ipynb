{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e08174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /scratchdata1/users/a1841503/Geneformer/Jupyter\n",
      "New directory: /scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check current directory\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# Change to your code directory (replace with your actual path)\n",
    "os.chdir('/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer/geneformer')\n",
    "\n",
    "# Verify the change\n",
    "print(\"New directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e0faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change to the repository root\n",
    "os.chdir(\"/scratchdata1/groups/phoenix-hpc-mangiola_laboratory/haroon/full_frozen_geneformer/Geneformer\")\n",
    "\n",
    "# Add current directory to Python path\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83607a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter kernelspec list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c033cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be50793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer.finetuner import FineTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba456902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer.finetuner_utils import get_train_valid_test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f66a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35870144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_config = {\n",
    "        \"tasks\": {\n",
    "        \"disease_classification\": [\n",
    "            # \"genecorpus_heart_disease\", \n",
    "            # \"cellnexus_blood_disease\", \n",
    "            \"cellnexus_covid_disease\"\n",
    "            ],\n",
    "        # \"dosage_sensitivity\": [\"genecorpus_dosage_sensitivity\"],\n",
    "        \"cell_type_classification\": [\"cellnexus_cell_types\"]\n",
    "        },\n",
    "        \"aggregation_levels\": [\n",
    "            \"singlecell\",\n",
    "            \"metacell_2\", \n",
    "            \"metacell_4\", \n",
    "            \"metacell_8\", \n",
    "            \"metacell_16\", \n",
    "            # \"metacell_32\", \n",
    "            # \"metacell_64\", \n",
    "            # \"metacell_128\"\n",
    "            ]\n",
    "    }\n",
    "\n",
    "# aggregation_level=\"metacell_32\"\n",
    "# task=\"dosage_sensitivity\"\n",
    "# dataset=\"genecorpus_dosage_sensitivity\"\n",
    "model_version=\"V1\"\n",
    "base_dir= \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer\"\n",
    "model_variant=\"30M\"\n",
    "crossval_splits = 5 # 1 or 5\n",
    "freeze_num_encoder_layers=6\n",
    "freeze_entire_model=True\n",
    "\n",
    "    # Loop over tasks\n",
    "for task, datasets in task_config[\"tasks\"].items():\n",
    "    # Loop over each dataset associated with the task\n",
    "    for dataset in datasets:\n",
    "        # Loop over aggregation levels\n",
    "        for aggregation_level in task_config[\"aggregation_levels\"]:\n",
    "            print(f\"Running task={task}, dataset={dataset}, aggregation_level={aggregation_level}\")\n",
    "\n",
    "            if task not in [\"dosage_sensitivity\", \"cell_type_classification\"]:\n",
    "                crossval_split_metrics = {}\n",
    "\n",
    "                for crossval_split in range(1, crossval_splits + 1):\n",
    "                    print(f\"Running cross-validation split {crossval_split} of {crossval_splits} for task={task}, dataset={dataset}, aggregation_level={aggregation_level}\")\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "                    training_args = {\n",
    "                    \"num_train_epochs\": 10,\n",
    "                    \"learning_rate\": 0.000804,\n",
    "                    \"lr_scheduler_type\": \"polynomial\",\n",
    "                    \"warmup_steps\": 1812,\n",
    "                    \"weight_decay\":0.258828,\n",
    "                    \"per_device_train_batch_size\": 128,\n",
    "                    \"seed\": 73,\n",
    "                    \"evaluation_strategy\":\"epoch\",        # Evaluate every epoch\n",
    "                    \"save_strategy\":\"epoch\",              # Save checkpoint every epoch\n",
    "                    \"metric_for_best_model\":\"eval_loss\",  # Metric to determine \"best\" model # Doc: https://huggingface.co/transformers/v3.5.1/main_classes/trainer.html#:~:text=after%20each%20evaluation.-,metric_for_best_model,-(str%2C\n",
    "                    \"greater_is_better\":False,            # For loss, lower is better\n",
    "                    \"load_best_model_at_end\":True,        # KEY: Load best model at the end\n",
    "                    \"save_total_limit\":3,                 # Keep only 3 best checkpoints\n",
    "                    # \"logging_dir\": os.path.normpath(\"D:/geneformer_finetuning/trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_metacell_8/250623_geneformer_cellClassifier_genecorpus_heart_disease_test/ksplit1/runs\"),\n",
    "                    }\n",
    "                    \n",
    "\n",
    "                    input_data_file, cell_state_dict, filter_data_dict, train_test_id_split_dict, train_valid_id_split_dict = get_train_valid_test_splits(\n",
    "                        TASK=task,\n",
    "                        DATASET=dataset,\n",
    "                        MODEL_VARIANT=model_variant,\n",
    "                        DATASET_PATH = os.path.join(base_dir, \"datasets\", task, dataset),\n",
    "                        CROSSVAL_SPLITS=crossval_splits,\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "                    finetuner = FineTuner(base_dir=base_dir,\n",
    "                        aggregation_level=aggregation_level,\n",
    "                        model_variant=model_variant,\n",
    "                        task=task,\n",
    "                        dataset=dataset,\n",
    "                        crossval_splits=crossval_splits,)\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    output_prefix = \"test\" if crossval_splits == 1 else \"ksplit\" + str(crossval_split)\n",
    "\n",
    "                    model_dir = os.path.join(finetuner.output_dir, f\"geneformer_cellClassifier_{output_prefix}\")\n",
    "                    existing_metrics_path = os.path.join(model_dir, f\"{output_prefix}_eval_metrics_dict.pkl\")\n",
    "                    \n",
    "                    if os.path.exists(existing_metrics_path):\n",
    "                        print(f\"Loading existing metrics for cross-validation split {crossval_split} from {existing_metrics_path}\")\n",
    "                        # FIXED: Variable name consistency\n",
    "                        with open(existing_metrics_path, \"rb\") as f:\n",
    "                            all_metrics = pickle.load(f)\n",
    "\n",
    "                    else:\n",
    "        \n",
    "                        all_metrics = finetuner.finetune_model(\n",
    "                            training_args=training_args, \n",
    "                            cell_state_dict = cell_state_dict, \n",
    "                            filter_data_dict = filter_data_dict, \n",
    "                            input_data_file = input_data_file, \n",
    "                            output_prefix = output_prefix, \n",
    "                            train_test_id_split_dict = train_test_id_split_dict[str(crossval_split)] if train_test_id_split_dict is not None else None, \n",
    "                            train_valid_id_split_dict = train_valid_id_split_dict[str(crossval_split)] if train_valid_id_split_dict is not None else None, \n",
    "                            num_crossval_splits= 1 if task not in [\"dosage_sensitivity\", \"cell_type_classification\"] else crossval_splits,\n",
    "                            freeze_num_encoder_layers=freeze_num_encoder_layers,\n",
    "                            freeze_entire_model=freeze_entire_model,\n",
    "                            split_sizes={\"train\": 0.8, \"valid\": 0.2, \"test\": 0.0}\n",
    "                            )\n",
    "                    \n",
    "                    crossval_split_metrics[str(crossval_split)] = all_metrics\n",
    "\n",
    "                metrics_path = os.path.join(finetuner.output_dir, \"metrics.pkl\")\n",
    "                with open(metrics_path, \"wb\") as f:\n",
    "                    pickle.dump(crossval_split_metrics, f)\n",
    "\n",
    "                print(f\"✅ Metrics saved to: {metrics_path}\")\n",
    "\n",
    "            elif task in [\"dosage_sensitivity\", \"cell_type_classification\"]:\n",
    "\n",
    "                print(f\"Running cross-validation for task={task}, dataset={dataset}, aggregation_level={aggregation_level}\")\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                training_args = {\n",
    "                \"num_train_epochs\": 10,\n",
    "                \"learning_rate\": 0.000804,\n",
    "                \"lr_scheduler_type\": \"polynomial\",\n",
    "                \"warmup_steps\": 1812,\n",
    "                \"weight_decay\":0.258828,\n",
    "                \"per_device_train_batch_size\": 128,\n",
    "                \"seed\": 73,\n",
    "                \"evaluation_strategy\":\"epoch\",        # Evaluate every epoch\n",
    "                \"save_strategy\":\"epoch\",              # Save checkpoint every epoch\n",
    "                \"metric_for_best_model\":\"eval_loss\",  # Metric to determine \"best\" model # Doc: https://huggingface.co/transformers/v3.5.1/main_classes/trainer.html#:~:text=after%20each%20evaluation.-,metric_for_best_model,-(str%2C\n",
    "                \"greater_is_better\":False,            # For loss, lower is better\n",
    "                \"load_best_model_at_end\":True,        # KEY: Load best model at the end\n",
    "                \"save_total_limit\":3,                 # Keep only 3 best checkpoints\n",
    "                # \"logging_dir\": os.path.normpath(\"D:/geneformer_finetuning/trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_metacell_8/250623_geneformer_cellClassifier_genecorpus_heart_disease_test/ksplit1/runs\"),\n",
    "                }\n",
    "                \n",
    "\n",
    "                input_data_file, cell_state_dict, filter_data_dict, train_test_id_split_dict, train_valid_id_split_dict = get_train_valid_test_splits(\n",
    "                    TASK=task,\n",
    "                    DATASET=dataset,\n",
    "                    MODEL_VARIANT=model_variant,\n",
    "                    DATASET_PATH = os.path.join(base_dir, \"datasets\", task, dataset)\n",
    ",\n",
    "                    CROSSVAL_SPLITS=crossval_splits,\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "                finetuner = FineTuner(base_dir=base_dir,\n",
    "                    aggregation_level=aggregation_level,\n",
    "                    model_variant=model_variant,\n",
    "                    task=task,\n",
    "                    dataset=dataset,\n",
    "                    crossval_splits=crossval_splits,)\n",
    "                \n",
    "                output_prefix = \"5fold\" if crossval_splits == 5 else \"test\"\n",
    "\n",
    "                metrics_path = os.path.join(finetuner.output_dir, \"metrics.pkl\")\n",
    "                if os.path.exists(metrics_path):\n",
    "                    print(f\"Loading existing metrics for cross-validation splits from {metrics_path}\")\n",
    "                    with open(metrics_path, \"rb\") as f:\n",
    "                        all_metrics = pickle.load(f)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Running finetuning for task={task}, dataset={dataset}, aggregation_level={aggregation_level}\")\n",
    "                \n",
    "                \n",
    "    \n",
    "                    all_metrics = finetuner.finetune_model(\n",
    "                        training_args=training_args, \n",
    "                        cell_state_dict = cell_state_dict, \n",
    "                        filter_data_dict = filter_data_dict, \n",
    "                        input_data_file = input_data_file, \n",
    "                        output_prefix = output_prefix, \n",
    "                        train_test_id_split_dict = train_test_id_split_dict[str(crossval_split)] if train_test_id_split_dict is not None else None, \n",
    "                        train_valid_id_split_dict = train_valid_id_split_dict[str(crossval_split)] if train_valid_id_split_dict is not None else None, \n",
    "                        num_crossval_splits= 1 if task not in [\"dosage_sensitivity\", \"cell_type_classification\"] else crossval_splits,\n",
    "                        freeze_num_encoder_layers=freeze_num_encoder_layers,\n",
    "                        freeze_entire_model=freeze_entire_model,\n",
    "                        split_sizes={\"train\": 0.8, \"valid\": 0.2, \"test\": 0.0}\n",
    "                        )\n",
    "                    \n",
    "\n",
    "                    with open(metrics_path, \"wb\") as f:\n",
    "                        pickle.dump(all_metrics, f)\n",
    "\n",
    "                    print(f\"✅ Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ff1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metrics_path, \"wb\") as f:\n",
    "    pickle.dump(all_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c490c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/5fold_cv_trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_singlecell/metrics.pkl\", \"rb\") as f:\n",
    "    crossval_split_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_split_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6344a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "def plot_cv_metrics_and_confusion_matrices(crossval_split_metrics: Dict[str, Dict[str, Any]], \n",
    "                                          normalize_cm: bool = False,\n",
    "                                          figsize: tuple = (16, 12),\n",
    "                                          y_min: Optional[float] = None,\n",
    "                                          y_max: Optional[float] = None,\n",
    "                                          row_spacing: float = 0.3,\n",
    "                                          show_annotations: bool = True,\n",
    "                                          save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot cross-validation metrics (accuracy and F1) as line plots and confusion matrices as subplots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crossval_split_metrics : dict\n",
    "        Dictionary containing metrics for each fold\n",
    "    normalize_cm : bool, default=False\n",
    "        Whether to normalize confusion matrices\n",
    "    figsize : tuple, default=(16, 12)\n",
    "        Figure size for the plot\n",
    "    y_min : float, optional\n",
    "        Minimum y-axis value for metrics plots\n",
    "    y_max : float, optional\n",
    "        Maximum y-axis value for metrics plots\n",
    "    row_spacing : float, default=0.3\n",
    "        Spacing between rows of subplots\n",
    "    show_annotations : bool, default=True\n",
    "        Whether to show value annotations on line plots\n",
    "    save_path : str, optional\n",
    "        Path to save the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract metrics for line plots\n",
    "    folds = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for fold, metrics in crossval_split_metrics.items():\n",
    "        folds.append(int(fold))\n",
    "        accuracies.append(metrics['acc'][0] if isinstance(metrics['acc'], list) else metrics['acc'])\n",
    "        f1_scores.append(metrics['macro_f1'][0] if isinstance(metrics['macro_f1'], list) else metrics['macro_f1'])\n",
    "    \n",
    "    # Sort by fold number\n",
    "    sorted_data = sorted(zip(folds, accuracies, f1_scores))\n",
    "    folds, accuracies, f1_scores = zip(*sorted_data)\n",
    "    \n",
    "    # Create subplots: 3 rows, 3 columns\n",
    "    # Row 1: Line plots (col 1-2), Row 2-3: Confusion matrices (2x3 grid)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create GridSpec for custom layout with adjustable spacing\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    gs = GridSpec(3, 3, figure=fig, height_ratios=[1, 1, 1], hspace=row_spacing, wspace=0.3)\n",
    "    \n",
    "    # Plot 1: Accuracy line plot\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(folds, accuracies, marker='o', linewidth=2, markersize=8, color='blue', label='Accuracy')\n",
    "    ax1.set_xlabel('Fold Number')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Accuracy Across Folds', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(folds)\n",
    "    \n",
    "    # Set y-axis limits if specified\n",
    "    if y_min is not None or y_max is not None:\n",
    "        ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Add value labels on points\n",
    "    if show_annotations:\n",
    "        for i, (fold, acc) in enumerate(zip(folds, accuracies)):\n",
    "            ax1.annotate(f'{acc:.3f}', (fold, acc), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: F1 Score line plot\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(folds, f1_scores, marker='s', linewidth=2, markersize=8, color='red', label='Macro F1')\n",
    "    ax2.set_xlabel('Fold Number')\n",
    "    ax2.set_ylabel('Macro F1 Score')\n",
    "    ax2.set_title('Macro F1 Score Across Folds', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(folds)\n",
    "    \n",
    "    # Set y-axis limits if specified\n",
    "    if y_min is not None or y_max is not None:\n",
    "        ax2.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Add value labels on points\n",
    "    if show_annotations:\n",
    "        for i, (fold, f1) in enumerate(zip(folds, f1_scores)):\n",
    "            ax2.annotate(f'{f1:.3f}', (fold, f1), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center', fontsize=9)\n",
    "    \n",
    "    # Plot 3: Combined metrics\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    line1 = ax3.plot(folds, accuracies, marker='o', linewidth=2, markersize=6, color='blue', label='Accuracy')\n",
    "    line2 = ax3.plot(folds, f1_scores, marker='s', linewidth=2, markersize=6, color='red', label='Macro F1')\n",
    "    ax3.set_xlabel('Fold Number')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_title('Combined Metrics', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xticks(folds)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Set y-axis limits if specified\n",
    "    if y_min is not None or y_max is not None:\n",
    "        ax3.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Add annotations for combined metrics\n",
    "    if show_annotations:\n",
    "        for i, (fold, acc, f1) in enumerate(zip(folds, accuracies, f1_scores)):\n",
    "            ax3.annotate(f'{acc:.3f}', (fold, acc), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center', fontsize=8, color='blue')\n",
    "            ax3.annotate(f'{f1:.3f}', (fold, f1), textcoords=\"offset points\", \n",
    "                        xytext=(0,-15), ha='center', fontsize=8, color='red')\n",
    "    \n",
    "    # Plot confusion matrices in 2x3 grid\n",
    "    cm_positions = [(1, 0), (1, 1), (1, 2), (2, 0), (2, 1)]\n",
    "    \n",
    "    for i, (fold, metrics) in enumerate(sorted(crossval_split_metrics.items(), key=lambda x: int(x[0]))):\n",
    "        if i >= 5:  # Only plot first 5 folds\n",
    "            break\n",
    "            \n",
    "        row, col = cm_positions[i]\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        # Extract confusion matrix\n",
    "        conf_matrix = metrics['conf_matrix']\n",
    "        \n",
    "        # Convert to numpy array if needed\n",
    "        if hasattr(conf_matrix, 'values'):\n",
    "            cm_array = conf_matrix.values\n",
    "            class_names = conf_matrix.index.tolist()\n",
    "        else:\n",
    "            # Assume it's already a DataFrame or array\n",
    "            cm_array = np.array(conf_matrix)\n",
    "            class_names = ['nf', 'hcm', 'dcm']  # Default class names\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if normalize_cm:\n",
    "            cm_array = cm_array.astype('float') / cm_array.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2f'\n",
    "            title_suffix = ' (Normalized)'\n",
    "        else:\n",
    "            fmt = '.0f'\n",
    "            title_suffix = ''\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(cm_array, annot=True, fmt=fmt, cmap='Blues', \n",
    "                   xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "        ax.set_title(f'Fold {fold} Confusion Matrix{title_suffix}', fontweight='bold')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle('Cross-Validation Results: Metrics and Confusion Matrices', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Add summary statistics as text\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    summary_text = f'Summary Statistics:\\n'\n",
    "    summary_text += f'Accuracy: {mean_acc:.3f} ± {std_acc:.3f}\\n'\n",
    "    summary_text += f'Macro F1: {mean_f1:.3f} ± {std_f1:.3f}'\n",
    "    \n",
    "    fig.text(0.98, 0.02, summary_text, fontsize=10, ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_average_metrics(crossval_split_metrics: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute average metrics across all cross-validation folds.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crossval_split_metrics : dict\n",
    "        Dictionary containing metrics for each fold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with averaged metrics: {'sum_conf_matrix', 'avg_macro_f1', 'avg_acc'}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract individual metrics\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for fold, metrics in crossval_split_metrics.items():\n",
    "        # Extract accuracy\n",
    "        acc = metrics['acc'][0] if isinstance(metrics['acc'], list) else metrics['acc']\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        # Extract F1 score\n",
    "        f1 = metrics['macro_f1'][0] if isinstance(metrics['macro_f1'], list) else metrics['macro_f1']\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # Extract confusion matrix\n",
    "        conf_matrix = metrics['conf_matrix']\n",
    "        if hasattr(conf_matrix, 'values'):\n",
    "            cm_array = conf_matrix.values\n",
    "        else:\n",
    "            cm_array = np.array(conf_matrix)\n",
    "        confusion_matrices.append(cm_array)\n",
    "    \n",
    "    # Compute averages for metrics and sum for confusion matrix\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    sum_confusion_matrix = np.sum(confusion_matrices, axis=0)  # Sum instead of mean\n",
    "    \n",
    "    # Create result dictionary\n",
    "    avg_metrics = {\n",
    "        'avg_acc': avg_accuracy,\n",
    "        'avg_macro_f1': avg_f1,\n",
    "        'sum_conf_matrix': sum_confusion_matrix  # Updated key name to reflect it's a sum\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Average Metrics Computed:\")\n",
    "    print(f\"   Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"   Average Macro F1: {avg_f1:.4f}\")\n",
    "    print(f\"   Summed Confusion Matrix shape: {sum_confusion_matrix.shape}\")\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def plot_average_metrics(avg_metrics: Dict[str, Any], \n",
    "                        class_names: list = ['nf', 'hcm', 'dcm'],\n",
    "                        normalize_cm: bool = False,\n",
    "                        figsize: tuple = (12, 5),\n",
    "                        y_min: Optional[float] = None,\n",
    "                        y_max: Optional[float] = None,\n",
    "                        show_annotations: bool = True,\n",
    "                        bar_width: float = 0.6,\n",
    "                        save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot average metrics: bar chart for accuracy/F1 and heatmap for confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    avg_metrics : dict\n",
    "        Dictionary containing averaged metrics\n",
    "    class_names : list, default=['nf', 'hcm', 'dcm']\n",
    "        Names of the classes for confusion matrix labels\n",
    "    normalize_cm : bool, default=False\n",
    "        Whether to normalize the confusion matrix\n",
    "    figsize : tuple, default=(12, 5)\n",
    "        Figure size for the plot\n",
    "    y_min : float, optional\n",
    "        Minimum y-axis value for bar chart\n",
    "    y_max : float, optional\n",
    "        Maximum y-axis value for bar chart\n",
    "    show_annotations : bool, default=True\n",
    "        Whether to show value annotations on bars\n",
    "    bar_width : float, default=0.6\n",
    "        Width of the bars in the bar chart\n",
    "    save_path : str, optional\n",
    "        Path to save the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Bar chart of average metrics\n",
    "    metrics_names = ['Accuracy', 'Macro F1']\n",
    "    metrics_values = [avg_metrics['avg_acc'], avg_metrics['avg_macro_f1']]\n",
    "    colors = ['skyblue', 'lightcoral']\n",
    "    \n",
    "    bars = ax1.bar(metrics_names, metrics_values, color=colors, alpha=0.8, \n",
    "                   edgecolor='black', width=bar_width)\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Average Cross-Validation Metrics', fontweight='bold')\n",
    "    \n",
    "    # Set y-axis limits if specified\n",
    "    if y_min is not None or y_max is not None:\n",
    "        ax1.set_ylim(y_min, y_max)\n",
    "    else:\n",
    "        ax1.set_ylim(0, 1)\n",
    "    \n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    if show_annotations:\n",
    "        for bar, value in zip(bars, metrics_values):\n",
    "            height = bar.get_height()\n",
    "            ax1.annotate(f'{value:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom',\n",
    "                        fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Average confusion matrix\n",
    "    cm_array = avg_metrics['sum_conf_matrix']  # Updated key name\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize_cm:\n",
    "        cm_array = cm_array.astype('float') / cm_array.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.3f'\n",
    "        title_suffix = ' (Normalized)'\n",
    "        cmap = 'Blues'\n",
    "    else:\n",
    "        fmt = '.0f'\n",
    "        title_suffix = ''\n",
    "        cmap = 'Blues'\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(cm_array, annot=True, fmt=fmt, cmap=cmap, \n",
    "               xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "    ax2.set_title(f'Summed Confusion Matrix{title_suffix}', fontweight='bold')  # Updated title\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_ylabel('Actual')\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_text = f'Cross-Validation Summary:\\n'\n",
    "    summary_text += f'Accuracy: {avg_metrics[\"avg_acc\"]:.4f}\\n'\n",
    "    summary_text += f'Macro F1: {avg_metrics[\"avg_macro_f1\"]:.4f}'\n",
    "    \n",
    "    fig.text(0.02, 0.98, summary_text, fontsize=10, ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "            transform=fig.transFigure)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Average metrics plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def comprehensive_cv_analysis(crossval_split_metrics: Dict[str, Dict[str, Any]],\n",
    "                             class_names: list = ['nf', 'hcm', 'dcm'],\n",
    "                             normalize_cm: bool = False,\n",
    "                             y_min: Optional[float] = None,\n",
    "                             y_max: Optional[float] = None,\n",
    "                             row_spacing: float = 0.3,\n",
    "                             show_annotations: bool = True,\n",
    "                             bar_width: float = 0.6,\n",
    "                             save_individual: Optional[str] = None,\n",
    "                             save_average: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis: plot individual fold results, compute averages, and plot averages.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crossval_split_metrics : dict\n",
    "        Dictionary containing metrics for each fold\n",
    "    class_names : list, default=['nf', 'hcm', 'dcm']\n",
    "        Names of the classes\n",
    "    normalize_cm : bool, default=False\n",
    "        Whether to normalize confusion matrices\n",
    "    y_min : float, optional\n",
    "        Minimum y-axis value for metrics plots\n",
    "    y_max : float, optional\n",
    "        Maximum y-axis value for metrics plots\n",
    "    row_spacing : float, default=0.3\n",
    "        Spacing between rows of subplots\n",
    "    show_annotations : bool, default=True\n",
    "        Whether to show value annotations on plots\n",
    "    bar_width : float, default=0.6\n",
    "        Width of the bars in average metrics bar chart\n",
    "    save_individual : str, optional\n",
    "        Path to save individual folds plot\n",
    "    save_average : str, optional\n",
    "        Path to save average metrics plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with averaged metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 Starting comprehensive cross-validation analysis...\")\n",
    "    \n",
    "    # Plot individual fold results\n",
    "    print(\"\\n📊 Plotting individual fold results...\")\n",
    "    plot_cv_metrics_and_confusion_matrices(\n",
    "        crossval_split_metrics, \n",
    "        normalize_cm=normalize_cm,\n",
    "        y_min=y_min,\n",
    "        y_max=y_max,\n",
    "        row_spacing=row_spacing,\n",
    "        show_annotations=show_annotations,\n",
    "        save_path=save_individual\n",
    "    )\n",
    "    \n",
    "    # Compute average metrics\n",
    "    print(\"\\n🧮 Computing average metrics...\")\n",
    "    avg_metrics = compute_average_metrics(crossval_split_metrics)\n",
    "    \n",
    "    # Plot average results\n",
    "    print(\"\\n📈 Plotting average results...\")\n",
    "    plot_average_metrics(\n",
    "        avg_metrics, \n",
    "        class_names=class_names, \n",
    "        normalize_cm=normalize_cm,\n",
    "        y_min=y_min,\n",
    "        y_max=y_max,\n",
    "        show_annotations=show_annotations,\n",
    "        bar_width=bar_width,\n",
    "        save_path=save_average\n",
    "    )\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Individual fold results\n",
    "    folds = sorted([int(k) for k in crossval_split_metrics.keys()])\n",
    "    print(f\"\\nIndividual Fold Results:\")\n",
    "    print(f\"{'Fold':<6}{'Accuracy':<12}{'Macro F1':<12}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for fold in folds:\n",
    "        fold_str = str(fold)\n",
    "        acc = crossval_split_metrics[fold_str]['acc'][0] if isinstance(crossval_split_metrics[fold_str]['acc'], list) else crossval_split_metrics[fold_str]['acc']\n",
    "        f1 = crossval_split_metrics[fold_str]['macro_f1'][0] if isinstance(crossval_split_metrics[fold_str]['macro_f1'], list) else crossval_split_metrics[fold_str]['macro_f1']\n",
    "        print(f\"{fold:<6}{acc:<12.4f}{f1:<12.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    accs = [crossval_split_metrics[str(f)]['acc'][0] if isinstance(crossval_split_metrics[str(f)]['acc'], list) else crossval_split_metrics[str(f)]['acc'] for f in folds]\n",
    "    f1s = [crossval_split_metrics[str(f)]['macro_f1'][0] if isinstance(crossval_split_metrics[str(f)]['macro_f1'], list) else crossval_split_metrics[str(f)]['macro_f1'] for f in folds]\n",
    "    \n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"Accuracy  - Mean: {np.mean(accs):.4f}, Std: {np.std(accs):.4f}, Min: {np.min(accs):.4f}, Max: {np.max(accs):.4f}\")\n",
    "    print(f\"Macro F1  - Mean: {np.mean(f1s):.4f}, Std: {np.std(f1s):.4f}, Min: {np.min(f1s):.4f}, Max: {np.max(f1s):.4f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Analysis completed successfully!\")\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed72d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic usage (same as before)\n",
    "avg_metrics = comprehensive_cv_analysis(crossval_split_metrics,\n",
    "                                        class_names = ['nf', 'hcm', 'dcm'],\n",
    "                             normalize_cm = True,\n",
    "                             y_min = 0.35,\n",
    "                             y_max = 0.65,\n",
    "                             row_spacing = 0.5,\n",
    "                             show_annotations = True,\n",
    "                             bar_width = 0.6,)\n",
    "\n",
    "# With custom parameters\n",
    "avg_metrics = comprehensive_cv_analysis(\n",
    "    crossval_split_metrics,\n",
    "    y_min=0.7,           # Set minimum y-axis value\n",
    "    y_max=1.0,           # Set maximum y-axis value\n",
    "    row_spacing=0.5,     # Increase spacing between rows\n",
    "    show_annotations=False,  # Hide annotations\n",
    "    bar_width=0.8,       # Wider bars\n",
    "    normalize_cm=True\n",
    ")\n",
    "\n",
    "# # Individual function calls with parameters\n",
    "# plot_cv_metrics_and_confusion_matrices(\n",
    "#     crossval_split_metrics,\n",
    "#     y_min=0.8,\n",
    "#     y_max=1.0,\n",
    "#     row_spacing=0.4,\n",
    "#     show_annotations=True\n",
    "# )\n",
    "\n",
    "# plot_average_metrics(\n",
    "#     avg_metrics,\n",
    "#     y_min=0.85,\n",
    "#     y_max=0.95,\n",
    "#     bar_width=0.5,\n",
    "#     show_annotations=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1cb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb69e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def plot_cv_metrics_and_confusion_matrices(crossval_split_metrics: Dict[str, Dict[str, Any]], \n",
    "                                          normalize_cm: bool = False,\n",
    "                                          figsize: tuple = (16, 12),\n",
    "                                          save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot cross-validation metrics (accuracy and F1) as line plots and confusion matrices as subplots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crossval_split_metrics : dict\n",
    "        Dictionary containing metrics for each fold\n",
    "    normalize_cm : bool, default=False\n",
    "        Whether to normalize confusion matrices\n",
    "    figsize : tuple, default=(16, 12)\n",
    "        Figure size for the plot\n",
    "    save_path : str, optional\n",
    "        Path to save the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract metrics for line plots\n",
    "    folds = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for fold, metrics in crossval_split_metrics.items():\n",
    "        folds.append(int(fold))\n",
    "        accuracies.append(metrics['acc'][0] if isinstance(metrics['acc'], list) else metrics['acc'])\n",
    "        f1_scores.append(metrics['macro_f1'][0] if isinstance(metrics['macro_f1'], list) else metrics['macro_f1'])\n",
    "    \n",
    "    # Sort by fold number\n",
    "    sorted_data = sorted(zip(folds, accuracies, f1_scores))\n",
    "    folds, accuracies, f1_scores = zip(*sorted_data)\n",
    "    \n",
    "    # Create subplots: 3 rows, 3 columns\n",
    "    # Row 1: Line plots (col 1-2), Row 2-3: Confusion matrices (2x3 grid)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create GridSpec for custom layout\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    gs = GridSpec(3, 3, figure=fig, height_ratios=[1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Plot 1: Accuracy line plot\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(folds, accuracies, marker='o', linewidth=2, markersize=8, color='blue', label='Accuracy')\n",
    "    ax1.set_xlabel('Fold Number')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Accuracy Across Folds', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(folds)\n",
    "    \n",
    "    # Add value labels on points\n",
    "    for i, (fold, acc) in enumerate(zip(folds, accuracies)):\n",
    "        ax1.annotate(f'{acc:.3f}', (fold, acc), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: F1 Score line plot\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(folds, f1_scores, marker='s', linewidth=2, markersize=8, color='red', label='Macro F1')\n",
    "    ax2.set_xlabel('Fold Number')\n",
    "    ax2.set_ylabel('Macro F1 Score')\n",
    "    ax2.set_title('Macro F1 Score Across Folds', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(folds)\n",
    "    \n",
    "    # Add value labels on points\n",
    "    for i, (fold, f1) in enumerate(zip(folds, f1_scores)):\n",
    "        ax2.annotate(f'{f1:.3f}', (fold, f1), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontsize=9)\n",
    "    \n",
    "    # Plot 3: Combined metrics\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.plot(folds, accuracies, marker='o', linewidth=2, markersize=6, color='blue', label='Accuracy')\n",
    "    ax3.plot(folds, f1_scores, marker='s', linewidth=2, markersize=6, color='red', label='Macro F1')\n",
    "    ax3.set_xlabel('Fold Number')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_title('Combined Metrics', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xticks(folds)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Plot confusion matrices in 2x3 grid\n",
    "    cm_positions = [(1, 0), (1, 1), (1, 2), (2, 0), (2, 1)]\n",
    "    \n",
    "    for i, (fold, metrics) in enumerate(sorted(crossval_split_metrics.items(), key=lambda x: int(x[0]))):\n",
    "        if i >= 5:  # Only plot first 5 folds\n",
    "            break\n",
    "            \n",
    "        row, col = cm_positions[i]\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        # Extract confusion matrix\n",
    "        conf_matrix = metrics['conf_matrix']\n",
    "        \n",
    "        # Convert to numpy array if needed\n",
    "        if hasattr(conf_matrix, 'values'):\n",
    "            cm_array = conf_matrix.values\n",
    "            class_names = conf_matrix.index.tolist()\n",
    "        else:\n",
    "            # Assume it's already a DataFrame or array\n",
    "            cm_array = np.array(conf_matrix)\n",
    "            class_names = ['nf', 'hcm', 'dcm']  # Default class names\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if normalize_cm:\n",
    "            cm_array = cm_array.astype('float') / cm_array.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2f'\n",
    "            title_suffix = ' (Normalized)'\n",
    "        else:\n",
    "            fmt = '.0f'\n",
    "            title_suffix = ''\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(cm_array, annot=True, fmt=fmt, cmap='Blues', \n",
    "                   xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "        ax.set_title(f'Fold {fold} Confusion Matrix{title_suffix}', fontweight='bold')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle('Cross-Validation Results: Metrics and Confusion Matrices', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Add summary statistics as text\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    summary_text = f'Summary Statistics:\\n'\n",
    "    summary_text += f'Accuracy: {mean_acc:.3f} ± {std_acc:.3f}\\n'\n",
    "    summary_text += f'Macro F1: {mean_f1:.3f} ± {std_f1:.3f}'\n",
    "    \n",
    "    fig.text(0.98, 0.02, summary_text, fontsize=10, ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_average_metrics(crossval_split_metrics: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute average metrics across all cross-validation folds.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crossval_split_metrics : dict\n",
    "        Dictionary containing metrics for each fold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with averaged metrics: {'avg_conf_matrix', 'avg_macro_f1', 'avg_acc'}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract individual metrics\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for fold, metrics in crossval_split_metrics.items():\n",
    "        # Extract accuracy\n",
    "        acc = metrics['acc'][0] if isinstance(metrics['acc'], list) else metrics['acc']\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        # Extract F1 score\n",
    "        f1 = metrics['macro_f1'][0] if isinstance(metrics['macro_f1'], list) else metrics['macro_f1']\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # Extract confusion matrix\n",
    "        conf_matrix = metrics['conf_matrix']\n",
    "        if hasattr(conf_matrix, 'values'):\n",
    "            cm_array = conf_matrix.values\n",
    "        else:\n",
    "            cm_array = np.array(conf_matrix)\n",
    "        confusion_matrices.append(cm_array)\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "    \n",
    "    # Create result dictionary\n",
    "    avg_metrics = {\n",
    "        'avg_acc': avg_accuracy,\n",
    "        'avg_macro_f1': avg_f1,\n",
    "        'avg_conf_matrix': avg_confusion_matrix\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Average Metrics Computed:\")\n",
    "    print(f\"   Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"   Average Macro F1: {avg_f1:.4f}\")\n",
    "    print(f\"   Average Confusion Matrix shape: {avg_confusion_matrix.shape}\")\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def plot_average_metrics(avg_metrics: Dict[str, Any], \n",
    "                        class_names: list = ['nf', 'hcm', 'dcm'],\n",
    "                        normalize_cm: bool = False,\n",
    "                        figsize: tuple = (12, 5),\n",
    "                        save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot average metrics: bar chart for accuracy/F1 and heatmap for confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    avg_metrics : dict\n",
    "        Dictionary containing averaged metrics\n",
    "    class_names : list, default=['nf', 'hcm', 'dcm']\n",
    "        Names of the classes for confusion matrix labels\n",
    "    normalize_cm : bool, default=False\n",
    "        Whether to normalize the confusion matrix\n",
    "    figsize : tuple, default=(12, 5)\n",
    "        Figure size for the plot\n",
    "    save_path : str, optional\n",
    "        Path to save the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Bar chart of average metrics\n",
    "    metrics_names = ['Accuracy', 'Macro F1']\n",
    "    metrics_values = [avg_metrics['avg_acc'], avg_metrics['avg_macro_f1']]\n",
    "    colors = ['skyblue', 'lightcoral']\n",
    "    \n",
    "    bars = ax1.bar(metrics_names, metrics_values, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Average Cross-Validation Metrics', fontweight='bold')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metrics_values):\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{value:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom',\n",
    "                    fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Average confusion matrix\n",
    "    cm_array = avg_metrics['avg_conf_matrix']\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize_cm:\n",
    "        cm_array = cm_array.astype('float') / cm_array.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.3f'\n",
    "        title_suffix = ' (Normalized)'\n",
    "        cmap = 'Blues'\n",
    "    else:\n",
    "        fmt = '.0f'\n",
    "        title_suffix = ''\n",
    "        cmap = 'Blues'\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(cm_array, annot=True, fmt=fmt, cmap=cmap, \n",
    "               xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "    ax2.set_title(f'Average Confusion Matrix{title_suffix}', fontweight='bold')\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_ylabel('Actual')\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_text = f'Cross-Validation Summary:\\n'\n",
    "    summary_text += f'Accuracy: {avg_metrics[\"avg_acc\"]:.4f}\\n'\n",
    "    summary_text += f'Macro F1: {avg_metrics[\"avg_macro_f1\"]:.4f}'\n",
    "    \n",
    "    fig.text(0.02, 0.98, summary_text, fontsize=10, ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "            transform=fig.transFigure)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Average metrics plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def comprehensive_cv_analysis(crossval_split_metrics: Dict[str, Dict[str, Any]],\n",
    "                             class_names: list = ['nf', 'hcm', 'dcm'],\n",
    "                             normalize_cm: bool = False,\n",
    "                             save_individual: Optional[str] = None,\n",
    "                             save_average: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis: plot individual fold results, compute averages, and plot averages.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    crossval_split_metrics : dict\n",
    "        Dictionary containing metrics for each fold\n",
    "    class_names : list, default=['nf', 'hcm', 'dcm']\n",
    "        Names of the classes\n",
    "    normalize_cm : bool, default=False\n",
    "        Whether to normalize confusion matrices\n",
    "    save_individual : str, optional\n",
    "        Path to save individual folds plot\n",
    "    save_average : str, optional\n",
    "        Path to save average metrics plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with averaged metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 Starting comprehensive cross-validation analysis...\")\n",
    "    \n",
    "    # Plot individual fold results\n",
    "    print(\"\\n📊 Plotting individual fold results...\")\n",
    "    plot_cv_metrics_and_confusion_matrices(\n",
    "        crossval_split_metrics, \n",
    "        normalize_cm=normalize_cm, \n",
    "        save_path=save_individual\n",
    "    )\n",
    "    \n",
    "    # Compute average metrics\n",
    "    print(\"\\n🧮 Computing average metrics...\")\n",
    "    avg_metrics = compute_average_metrics(crossval_split_metrics)\n",
    "    \n",
    "    # Plot average results\n",
    "    print(\"\\n📈 Plotting average results...\")\n",
    "    plot_average_metrics(\n",
    "        avg_metrics, \n",
    "        class_names=class_names, \n",
    "        normalize_cm=normalize_cm, \n",
    "        save_path=save_average\n",
    "    )\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Individual fold results\n",
    "    folds = sorted([int(k) for k in crossval_split_metrics.keys()])\n",
    "    print(f\"\\nIndividual Fold Results:\")\n",
    "    print(f\"{'Fold':<6}{'Accuracy':<12}{'Macro F1':<12}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for fold in folds:\n",
    "        fold_str = str(fold)\n",
    "        acc = crossval_split_metrics[fold_str]['acc'][0] if isinstance(crossval_split_metrics[fold_str]['acc'], list) else crossval_split_metrics[fold_str]['acc']\n",
    "        f1 = crossval_split_metrics[fold_str]['macro_f1'][0] if isinstance(crossval_split_metrics[fold_str]['macro_f1'], list) else crossval_split_metrics[fold_str]['macro_f1']\n",
    "        print(f\"{fold:<6}{acc:<12.4f}{f1:<12.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    accs = [crossval_split_metrics[str(f)]['acc'][0] if isinstance(crossval_split_metrics[str(f)]['acc'], list) else crossval_split_metrics[str(f)]['acc'] for f in folds]\n",
    "    f1s = [crossval_split_metrics[str(f)]['macro_f1'][0] if isinstance(crossval_split_metrics[str(f)]['macro_f1'], list) else crossval_split_metrics[str(f)]['macro_f1'] for f in folds]\n",
    "    \n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"Accuracy  - Mean: {np.mean(accs):.4f}, Std: {np.std(accs):.4f}, Min: {np.min(accs):.4f}, Max: {np.max(accs):.4f}\")\n",
    "    print(f\"Macro F1  - Mean: {np.mean(f1s):.4f}, Std: {np.std(f1s):.4f}, Min: {np.min(f1s):.4f}, Max: {np.max(f1s):.4f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Analysis completed successfully!\")\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    avg_results = comprehensive_cv_analysis(\n",
    "        crossval_split_metrics,\n",
    "        class_names=['nf', 'hcm', 'dcm'],\n",
    "        normalize_cm=False,\n",
    "        save_individual=\"cv_individual_results.png\",\n",
    "        save_average=\"cv_average_results.png\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎯 Average metrics dictionary:\")\n",
    "    for key, value in avg_results.items():\n",
    "        if key == 'avg_conf_matrix':\n",
    "            print(f\"{key}: shape {value.shape}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669dff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5100a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d23cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff95c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b27ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046994b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4264be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def analyze_cell_types_by_individual(dataset_path):\n",
    "    \"\"\"\n",
    "    Analyzes cell type diversity for each individual in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the HuggingFace dataset\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Summary statistics of cell type diversity\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading HuggingFace dataset...\")\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        dataset = Dataset.load_from_disk(dataset_path)\n",
    "        df = dataset.to_pandas()\n",
    "        print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_cols = ['individual', 'cell_type']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"❌ Missing columns: {missing_cols}\")\n",
    "            print(f\"Available columns: {df.columns.tolist()}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"CELL TYPE ANALYSIS BY INDIVIDUAL\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get unique individuals\n",
    "        unique_individuals = df['individual'].unique()\n",
    "        print(f\"Total unique individuals: {len(unique_individuals)}\")\n",
    "        \n",
    "        # Analyze cell type diversity for each individual\n",
    "        individual_analysis = []\n",
    "        \n",
    "        print(f\"\\nCell type diversity per individual:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for individual_id in sorted(unique_individuals):\n",
    "            # Get all samples for this individual\n",
    "            individual_data = df[df['individual'] == individual_id]\n",
    "            \n",
    "            # Get unique cell types for this individual\n",
    "            unique_cell_types = individual_data['cell_type'].unique()\n",
    "            num_unique_cell_types = len(unique_cell_types)\n",
    "            total_samples = len(individual_data)\n",
    "            \n",
    "            # Get cell type distribution\n",
    "            cell_type_counts = individual_data['cell_type'].value_counts()\n",
    "            \n",
    "            # Store summary info\n",
    "            individual_analysis.append({\n",
    "                'individual_id': individual_id,\n",
    "                'total_samples': total_samples,\n",
    "                'unique_cell_types': num_unique_cell_types,\n",
    "                'cell_type_list': list(unique_cell_types),\n",
    "                'most_common_cell_type': cell_type_counts.index[0],\n",
    "                'most_common_count': cell_type_counts.iloc[0],\n",
    "                'most_common_percentage': (cell_type_counts.iloc[0] / total_samples) * 100\n",
    "            })\n",
    "            \n",
    "            print(f\"Individual {individual_id}:\")\n",
    "            print(f\"  Total samples: {total_samples:,}\")\n",
    "            print(f\"  Unique cell types: {num_unique_cell_types}\")\n",
    "            print(f\"  Cell types: {', '.join(sorted(unique_cell_types))}\")\n",
    "            print(f\"  Most common: {cell_type_counts.index[0]} ({cell_type_counts.iloc[0]:,} samples, {(cell_type_counts.iloc[0]/total_samples)*100:.1f}%)\")\n",
    "            \n",
    "            # Show detailed distribution if there are multiple cell types\n",
    "            if num_unique_cell_types > 1:\n",
    "                print(f\"  Distribution:\")\n",
    "                for cell_type, count in cell_type_counts.items():\n",
    "                    percentage = (count / total_samples) * 100\n",
    "                    print(f\"    {cell_type}: {count:,} ({percentage:.1f}%)\")\n",
    "            print()\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        analysis_df = pd.DataFrame(individual_analysis)\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nCell type diversity statistics:\")\n",
    "        print(f\"  Min cell types per individual: {analysis_df['unique_cell_types'].min()}\")\n",
    "        print(f\"  Max cell types per individual: {analysis_df['unique_cell_types'].max()}\")\n",
    "        print(f\"  Mean cell types per individual: {analysis_df['unique_cell_types'].mean():.2f}\")\n",
    "        print(f\"  Median cell types per individual: {analysis_df['unique_cell_types'].median():.1f}\")\n",
    "        \n",
    "        # Distribution of cell type counts\n",
    "        cell_type_dist = analysis_df['unique_cell_types'].value_counts().sort_index()\n",
    "        print(f\"\\nDistribution of cell type diversity:\")\n",
    "        for num_types, count in cell_type_dist.items():\n",
    "            percentage = (count / len(analysis_df)) * 100\n",
    "            print(f\"  {num_types} cell types: {count} individuals ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Most common cell types across all individuals\n",
    "        print(f\"\\nMost common cell types across all individuals:\")\n",
    "        most_common_overall = analysis_df['most_common_cell_type'].value_counts()\n",
    "        for cell_type, count in most_common_overall.items():\n",
    "            percentage = (count / len(analysis_df)) * 100\n",
    "            print(f\"  {cell_type}: dominant in {count} individuals ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Get all unique cell types in the dataset\n",
    "        all_cell_types = df['cell_type'].unique()\n",
    "        print(f\"\\nOverall dataset cell type information:\")\n",
    "        print(f\"  Total unique cell types in dataset: {len(all_cell_types)}\")\n",
    "        print(f\"  All cell types: {', '.join(sorted(all_cell_types))}\")\n",
    "        \n",
    "        # Cell type frequency across the entire dataset\n",
    "        print(f\"\\nCell type frequency across entire dataset:\")\n",
    "        overall_cell_type_counts = df['cell_type'].value_counts()\n",
    "        total_samples_dataset = len(df)\n",
    "        for cell_type, count in overall_cell_type_counts.items():\n",
    "            percentage = (count / total_samples_dataset) * 100\n",
    "            print(f\"  {cell_type}: {count:,} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"CREATING VISUALIZATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Set up plotting\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Cell Type Analysis by Individual', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Distribution of cell type diversity\n",
    "        ax1 = axes[0, 0]\n",
    "        cell_type_dist.plot(kind='bar', ax=ax1, color='skyblue', alpha=0.7)\n",
    "        ax1.set_title('Distribution of Cell Type Diversity per Individual', fontweight='bold')\n",
    "        ax1.set_xlabel('Number of Unique Cell Types')\n",
    "        ax1.set_ylabel('Number of Individuals')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(cell_type_dist.values):\n",
    "            ax1.text(i, v + 0.1, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Plot 2: Sample count vs cell type diversity\n",
    "        ax2 = axes[0, 1]\n",
    "        scatter = ax2.scatter(analysis_df['total_samples'], analysis_df['unique_cell_types'], \n",
    "                             alpha=0.6, s=60, c=analysis_df['unique_cell_types'], \n",
    "                             cmap='viridis')\n",
    "        ax2.set_title('Sample Count vs Cell Type Diversity', fontweight='bold')\n",
    "        ax2.set_xlabel('Total Samples per Individual')\n",
    "        ax2.set_ylabel('Number of Unique Cell Types')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax2, label='Cell Type Count')\n",
    "        \n",
    "        # Plot 3: Most common cell types across individuals\n",
    "        ax3 = axes[1, 0]\n",
    "        most_common_overall.plot(kind='bar', ax=ax3, color='lightcoral', alpha=0.7)\n",
    "        ax3.set_title('Most Dominant Cell Type per Individual', fontweight='bold')\n",
    "        ax3.set_xlabel('Cell Type')\n",
    "        ax3.set_ylabel('Number of Individuals')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(most_common_overall.values):\n",
    "            ax3.text(i, v + 0.1, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Plot 4: Overall cell type distribution in dataset\n",
    "        ax4 = axes[1, 1]\n",
    "        # For better visualization, show top 10 cell types\n",
    "        top_cell_types = overall_cell_type_counts.head(10)\n",
    "        top_cell_types.plot(kind='bar', ax=ax4, color='lightgreen', alpha=0.7)\n",
    "        ax4.set_title('Top 10 Cell Types in Entire Dataset', fontweight='bold')\n",
    "        ax4.set_xlabel('Cell Type')\n",
    "        ax4.set_ylabel('Number of Samples')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, (cell_type, count) in enumerate(top_cell_types.items()):\n",
    "            pct = (count / total_samples_dataset) * 100\n",
    "            ax4.text(i, count + count*0.01, f'{pct:.1f}%', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/cell_type_analysis.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Visualizations saved to: {plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Save detailed results\n",
    "        csv_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/cell_type_analysis_detailed.csv\"\n",
    "        analysis_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Detailed analysis saved to: {csv_path}\")\n",
    "        \n",
    "        print(f\"\\n✅ Cell type analysis completed successfully!\")\n",
    "        return analysis_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/human_dcm_hcm_nf.dataset/\"\n",
    "    \n",
    "    print(\"Starting cell type analysis...\")\n",
    "    results = analyze_cell_types_by_individual(dataset_path)\n",
    "    \n",
    "    if results is not None:\n",
    "        print(f\"\\nAnalysis completed! Results shape: {results.shape}\")\n",
    "        print(\"\\nSample of results:\")\n",
    "        print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Read the HuggingFace dataset\n",
    "dataset_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/human_dcm_hcm_nf.dataset/\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset = Dataset.load_from_disk(dataset_path)\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'individual' not in df.columns or 'disease' not in df.columns:\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        print(\"Please check if 'individual' and 'disease' columns exist with these exact names\")\n",
    "    else:\n",
    "        print(\"=== ANALYSIS RESULTS ===\")\n",
    "        print()\n",
    "        \n",
    "        # 1. Number of unique individuals\n",
    "        unique_individuals = df['individual'].nunique()\n",
    "        print(f\"Number of unique individuals: {unique_individuals}\")\n",
    "        print()\n",
    "        \n",
    "        # 2. List all unique individuals\n",
    "        individual_ids = df['individual'].unique()\n",
    "        print(\"Unique individual IDs:\")\n",
    "        for i, ind_id in enumerate(sorted(individual_ids), 1):\n",
    "            print(f\"  {i}. {ind_id}\")\n",
    "        print()\n",
    "        \n",
    "        # 3. Overall disease distribution\n",
    "        disease_counts = df['disease'].value_counts()\n",
    "        print(\"Overall disease distribution:\")\n",
    "        for disease, count in disease_counts.items():\n",
    "            print(f\"  {disease}: {count} samples\")\n",
    "        print()\n",
    "        \n",
    "        # 4. Disease distribution per individual\n",
    "        print(\"Disease distribution per individual:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for ind_id in sorted(individual_ids):\n",
    "            individual_data = df[df['individual'] == ind_id]\n",
    "            disease_dist = individual_data['disease'].value_counts()\n",
    "            total_samples = len(individual_data)\n",
    "            \n",
    "            print(f\"\\nIndividual ID: {ind_id}\")\n",
    "            print(f\"Total samples: {total_samples}\")\n",
    "            print(\"Disease distribution:\")\n",
    "            for disease, count in disease_dist.items():\n",
    "                percentage = (count / total_samples) * 100\n",
    "                print(f\"  - {disease}: {count} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 5. Summary statistics\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Samples per individual\n",
    "        samples_per_individual = df.groupby('individual').size()\n",
    "        print(f\"\\nSamples per individual:\")\n",
    "        print(f\"  Min: {samples_per_individual.min()}\")\n",
    "        print(f\"  Max: {samples_per_individual.max()}\")\n",
    "        print(f\"  Mean: {samples_per_individual.mean():.1f}\")\n",
    "        print(f\"  Median: {samples_per_individual.median():.1f}\")\n",
    "        \n",
    "        # Cross-tabulation\n",
    "        print(f\"\\nCross-tabulation (Individual vs Disease):\")\n",
    "        crosstab = pd.crosstab(df['individual'], df['disease'], margins=True)\n",
    "        print(crosstab)\n",
    "        \n",
    "        # 6. Check for any individuals with multiple diseases\n",
    "        print(f\"\\nIndividuals with multiple disease types:\")\n",
    "        multi_disease_individuals = df.groupby('individual')['disease'].nunique()\n",
    "        multi_disease = multi_disease_individuals[multi_disease_individuals > 1]\n",
    "        \n",
    "        if len(multi_disease) > 0:\n",
    "            for ind_id, num_diseases in multi_disease.items():\n",
    "                diseases = df[df['individual'] == ind_id]['disease'].unique()\n",
    "                print(f\"  {ind_id}: {num_diseases} diseases - {list(diseases)}\")\n",
    "        else:\n",
    "            print(\"  No individuals have multiple disease types\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {str(e)}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"1. Check if the path exists\")\n",
    "    print(\"2. Verify the dataset format\")\n",
    "    print(\"3. Check column names\")\n",
    "    \n",
    "    # Try to list directory contents\n",
    "    try:\n",
    "        if os.path.exists(dataset_path):\n",
    "            contents = os.listdir(dataset_path)\n",
    "            print(f\"Directory contents: {contents}\")\n",
    "        else:\n",
    "            print(\"Dataset path does not exist\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Cannot access directory: {str(e2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Individual IDs and their diseases from your data\n",
    "individuals_diseases = {\n",
    "    \"1290\": 'dcm', \"1300\": 'dcm', \"1304\": 'dcm', \"1358\": 'dcm', \"1371\": 'dcm',\n",
    "    \"1430\": 'dcm', \"1437\": 'dcm', \"1472\": 'dcm', \"1504\": 'dcm', \"1606\": 'dcm',\n",
    "    \"1617\": 'dcm', \"1422\": 'hcm', \"1425\": 'hcm', \"1447\": 'hcm', \"1462\": 'hcm',\n",
    "    '1479': 'hcm', \"1508\": 'hcm', \"1510\": 'hcm', \"1602\": 'hcm', \"1630\": 'hcm',\n",
    "    \"1631\": 'hcm', \"1685\": 'hcm', \"1707\": 'hcm', \"1722\": 'hcm', \"1726\": 'hcm',\n",
    "    \"1735\": 'hcm', \"1515\": 'nf', \"1516\": 'nf', \"1539\": 'nf', \"1540\": 'nf',\n",
    "    \"1547\": 'nf', \"1549\": 'nf', \"1558\": 'nf', \"1561\": 'nf', \"1582\": 'nf',\n",
    "    \"1600\": 'nf', \"1603\": 'nf', \"1610\": 'nf', \"1622\": 'nf', \"1678\": 'nf',\n",
    "    \"1702\": 'nf', \"1718\": 'nf'\n",
    "}\n",
    "\n",
    "# Group individuals by disease\n",
    "disease_groups = defaultdict(list)\n",
    "for individual_id, disease in individuals_diseases.items():\n",
    "    disease_groups[disease].append(individual_id)\n",
    "\n",
    "print(\"Disease distribution:\")\n",
    "for disease, individuals in disease_groups.items():\n",
    "    print(f\"{disease}: {len(individuals)} individuals\")\n",
    "\n",
    "# Create 5 folds with balanced disease distribution\n",
    "def create_balanced_folds(disease_groups, n_folds=5):\n",
    "    folds = [[] for _ in range(n_folds)]\n",
    "    \n",
    "    # For each disease, distribute individuals across folds\n",
    "    for disease, individuals in disease_groups.items():\n",
    "        # Shuffle individuals within each disease group\n",
    "        shuffled_individuals = individuals.copy()\n",
    "        random.shuffle(shuffled_individuals)\n",
    "        \n",
    "        # Distribute individuals across folds in round-robin fashion\n",
    "        for i, individual in enumerate(shuffled_individuals):\n",
    "            fold_idx = i % n_folds\n",
    "            folds[fold_idx].append(individual)\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# Create the folds\n",
    "folds = create_balanced_folds(disease_groups)\n",
    "\n",
    "# Print fold statistics\n",
    "print(\"\\nFold statistics:\")\n",
    "for i, fold in enumerate(folds):\n",
    "    fold_diseases = [individuals_diseases[ind] for ind in fold]\n",
    "    disease_counts = {disease: fold_diseases.count(disease) for disease in ['dcm', 'hcm', 'nf']}\n",
    "    print(f\"Fold {i+1}: {len(fold)} individuals - DCM: {disease_counts['dcm']}, HCM: {disease_counts['hcm']}, NF: {disease_counts['nf']}\")\n",
    "\n",
    "# Create the cross-validation splits in the required format\n",
    "cv_splits = {}\n",
    "\n",
    "for fold_idx in range(5):\n",
    "    # Current fold becomes validation set\n",
    "    eval_individuals = folds[fold_idx]\n",
    "    \n",
    "    # All other folds become training set\n",
    "    train_individuals = []\n",
    "    for other_fold_idx in range(5):\n",
    "        if other_fold_idx != fold_idx:\n",
    "            train_individuals.extend(folds[other_fold_idx])\n",
    "    \n",
    "    cv_splits[str(fold_idx + 1)] = {\n",
    "        \"attr_key\": \"individual\",\n",
    "        \"train\": train_individuals,\n",
    "        \"eval\": eval_individuals\n",
    "    }\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/5fold_cv_splits.json\"\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(cv_splits, f, indent=2)\n",
    "\n",
    "print(f\"\\n5-fold cross-validation splits saved to: {output_path}\")\n",
    "\n",
    "# Print summary of each fold\n",
    "print(\"\\nDetailed fold information:\")\n",
    "for fold_num, fold_data in cv_splits.items():\n",
    "    print(f\"\\nFold {fold_num}:\")\n",
    "    print(f\"  Training set: {len(fold_data['train'])} individuals\")\n",
    "    print(f\"  Validation set: {len(fold_data['eval'])} individuals\")\n",
    "    \n",
    "    # Count diseases in training set\n",
    "    train_diseases = [individuals_diseases[ind] for ind in fold_data['train']]\n",
    "    train_disease_counts = {disease: train_diseases.count(disease) for disease in ['dcm', 'hcm', 'nf']}\n",
    "    print(f\"  Training diseases - DCM: {train_disease_counts['dcm']}, HCM: {train_disease_counts['hcm']}, NF: {train_disease_counts['nf']}\")\n",
    "    \n",
    "    # Count diseases in validation set\n",
    "    eval_diseases = [individuals_diseases[ind] for ind in fold_data['eval']]\n",
    "    eval_disease_counts = {disease: eval_diseases.count(disease) for disease in ['dcm', 'hcm', 'nf']}\n",
    "    print(f\"  Validation diseases - DCM: {eval_disease_counts['dcm']}, HCM: {eval_disease_counts['hcm']}, NF: {eval_disease_counts['nf']}\")\n",
    "\n",
    "print(\"\\nCross-validation splits created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/human_dcm_hcm_nf.dataset/\"\n",
    "json_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/5fold_cv_splits.json\"\n",
    "\n",
    "print(\"Loading HuggingFace dataset...\")\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset = Dataset.load_from_disk(dataset_path)\n",
    "    df = dataset.to_pandas()\n",
    "    print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Load JSON splits\n",
    "    print(\"\\nLoading cross-validation splits...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        cv_splits = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(cv_splits)} folds\")\n",
    "    \n",
    "    # Check for data leakage across all folds\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHECKING FOR DATA LEAKAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_train_ids = set()\n",
    "    all_eval_ids = set()\n",
    "    leakage_found = False\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        train_ids = set(fold_data['train'])\n",
    "        eval_ids = set(fold_data['eval'])\n",
    "        \n",
    "        # Check for overlap within the same fold\n",
    "        overlap = train_ids.intersection(eval_ids)\n",
    "        if overlap:\n",
    "            print(f\"❌ LEAKAGE DETECTED in Fold {fold_num}: {len(overlap)} individuals in both train and eval\")\n",
    "            print(f\"   Overlapping IDs: {list(overlap)}\")\n",
    "            leakage_found = True\n",
    "        else:\n",
    "            print(f\"✅ Fold {fold_num}: No overlap between train and eval sets\")\n",
    "        \n",
    "        all_train_ids.update(train_ids)\n",
    "        all_eval_ids.update(eval_ids)\n",
    "    \n",
    "    # Check if any individual appears in multiple eval sets\n",
    "    eval_counts = Counter()\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        for individual in fold_data['eval']:\n",
    "            eval_counts[individual] += 1\n",
    "    \n",
    "    multi_eval = {ind: count for ind, count in eval_counts.items() if count > 1}\n",
    "    if multi_eval:\n",
    "        print(f\"❌ LEAKAGE DETECTED: {len(multi_eval)} individuals appear in multiple eval sets\")\n",
    "        for ind, count in multi_eval.items():\n",
    "            print(f\"   Individual {ind} appears in {count} eval sets\")\n",
    "        leakage_found = True\n",
    "    \n",
    "    if not leakage_found:\n",
    "        print(\"✅ NO DATA LEAKAGE DETECTED: All splits are properly separated\")\n",
    "    \n",
    "    print(f\"\\nTotal unique individuals in training across all folds: {len(all_train_ids)}\")\n",
    "    print(f\"Total unique individuals in evaluation across all folds: {len(all_eval_ids)}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create subplots: 2 rows, 3 columns (5 folds + 1 overall)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Class Distribution Across 5-Fold Cross-Validation Splits', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Define colors for consistency\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']  # Red, Teal, Blue\n",
    "    disease_order = ['dcm', 'hcm', 'nf']\n",
    "    \n",
    "    # Plot each fold\n",
    "    for i, (fold_num, fold_data) in enumerate(cv_splits.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get data for this fold\n",
    "        train_df = df[df['individual'].isin(fold_data['train'])]\n",
    "        eval_df = df[df['individual'].isin(fold_data['eval'])]\n",
    "        \n",
    "        # Count samples by disease\n",
    "        train_counts = train_df['disease'].value_counts()\n",
    "        eval_counts = eval_df['disease'].value_counts()\n",
    "        \n",
    "        # Ensure all diseases are represented (even if count is 0)\n",
    "        train_counts = train_counts.reindex(disease_order, fill_value=0)\n",
    "        eval_counts = eval_counts.reindex(disease_order, fill_value=0)\n",
    "        \n",
    "        # Create bar plot\n",
    "        x = np.arange(len(disease_order))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, train_counts.values, width, label='Train', \n",
    "                      color=colors, alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, eval_counts.values, width, label='Eval', \n",
    "                      color=colors, alpha=0.5)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Customize subplot\n",
    "        ax.set_title(f'Fold {fold_num}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Disease Type', fontsize=10)\n",
    "        ax.set_ylabel('Number of Samples', fontsize=10)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(disease_order, fontsize=10)\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add fold statistics as text\n",
    "        train_total = len(train_df)\n",
    "        eval_total = len(eval_df)\n",
    "        train_individuals = len(fold_data['train'])\n",
    "        eval_individuals = len(fold_data['eval'])\n",
    "        \n",
    "        stats_text = f'Train: {train_total:,} samples ({train_individuals} individuals)\\n'\n",
    "        stats_text += f'Eval: {eval_total:,} samples ({eval_individuals} individuals)'\n",
    "        \n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=8)\n",
    "    \n",
    "    # Create overall statistics plot in the last subplot\n",
    "    ax_overall = axes[5]\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    overall_train_counts = df['disease'].value_counts().reindex(disease_order, fill_value=0)\n",
    "    \n",
    "    # Calculate average eval counts across folds\n",
    "    avg_eval_counts = []\n",
    "    for disease in disease_order:\n",
    "        total_eval = sum(df[df['individual'].isin(fold_data['eval'])]['disease'].value_counts().get(disease, 0) \n",
    "                        for fold_data in cv_splits.values())\n",
    "        avg_eval_counts.append(total_eval / len(cv_splits))\n",
    "    \n",
    "    # Plot overall statistics\n",
    "    x = np.arange(len(disease_order))\n",
    "    bars1 = ax_overall.bar(x - width/2, overall_train_counts.values, width, \n",
    "                          label='Total Dataset', color=colors, alpha=0.8)\n",
    "    bars2 = ax_overall.bar(x + width/2, avg_eval_counts, width, \n",
    "                          label='Avg Eval per Fold', color=colors, alpha=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax_overall.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax_overall.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax_overall.set_title('Overall Dataset Statistics', fontsize=12, fontweight='bold')\n",
    "    ax_overall.set_xlabel('Disease Type', fontsize=10)\n",
    "    ax_overall.set_ylabel('Number of Samples', fontsize=10)\n",
    "    ax_overall.set_xticks(x)\n",
    "    ax_overall.set_xticklabels(disease_order, fontsize=10)\n",
    "    ax_overall.legend(fontsize=9)\n",
    "    ax_overall.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add overall statistics text\n",
    "    total_samples = len(df)\n",
    "    total_individuals = df['individual'].nunique()\n",
    "    stats_text = f'Total: {total_samples:,} samples\\n'\n",
    "    stats_text += f'Individuals: {total_individuals}\\n'\n",
    "    stats_text += f'Avg per fold: {total_samples//5:,} samples'\n",
    "    \n",
    "    ax_overall.text(0.02, 0.98, stats_text, transform=ax_overall.transAxes, \n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                   fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/genecorpus_heart_disease/cv_splits_visualization.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Visualization saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED FOLD STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        print(f\"\\nFold {fold_num}:\")\n",
    "        \n",
    "        train_df = df[df['individual'].isin(fold_data['train'])]\n",
    "        eval_df = df[df['individual'].isin(fold_data['eval'])]\n",
    "        \n",
    "        print(f\"  Training:\")\n",
    "        print(f\"    Individuals: {len(fold_data['train'])}\")\n",
    "        print(f\"    Samples: {len(train_df):,}\")\n",
    "        train_dist = train_df['disease'].value_counts()\n",
    "        for disease in disease_order:\n",
    "            count = train_dist.get(disease, 0)\n",
    "            pct = (count / len(train_df)) * 100 if len(train_df) > 0 else 0\n",
    "            print(f\"    {disease.upper()}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"  Evaluation:\")\n",
    "        print(f\"    Individuals: {len(fold_data['eval'])}\")\n",
    "        print(f\"    Samples: {len(eval_df):,}\")\n",
    "        eval_dist = eval_df['disease'].value_counts()\n",
    "        for disease in disease_order:\n",
    "            count = eval_dist.get(disease, 0)\n",
    "            pct = (count / len(eval_df)) * 100 if len(eval_df) > 0 else 0\n",
    "            print(f\"    {disease.upper()}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n✅ Analysis completed successfully!\")\n",
    "    print(f\"📊 Visualization saved to: {plot_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ed1dd",
   "metadata": {},
   "source": [
    "# COVID Dataset from CellNexus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66242432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Read the HuggingFace dataset\n",
    "dataset_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/tokenized_30M/cellnexus_singlecell.dataset/\"\n",
    "\n",
    "print(\"Loading CellNexus COVID dataset...\")\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset = Dataset.load_from_disk(dataset_path)\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Dataset length: {len(dataset):,}\")\n",
    "    print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    df = dataset.to_pandas()\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'dataset_id' not in df.columns or 'disease' not in df.columns:\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        print(\"Please check if 'dataset_id' and 'disease' columns exist with these exact names\")\n",
    "        \n",
    "        # Show first few rows to understand the structure\n",
    "        print(\"\\nFirst 5 rows of the dataset:\")\n",
    "        print(df.head())\n",
    "        \n",
    "    else:\n",
    "        print(\"=== ANALYSIS RESULTS ===\")\n",
    "        print()\n",
    "        \n",
    "        # 1. Number of unique dataset_ids\n",
    "        unique_datasets = df['dataset_id'].nunique()\n",
    "        print(f\"Number of unique dataset IDs: {unique_datasets}\")\n",
    "        print()\n",
    "        \n",
    "        # 2. List all unique dataset_ids\n",
    "        dataset_ids = df['dataset_id'].unique()\n",
    "        print(\"Unique dataset IDs:\")\n",
    "        for i, dataset_id in enumerate(sorted(dataset_ids), 1):\n",
    "            print(f\"  {i:2d}. {dataset_id}\")\n",
    "        print()\n",
    "        \n",
    "        # 3. Overall disease distribution\n",
    "        disease_counts = df['disease'].value_counts()\n",
    "        print(\"Overall disease distribution:\")\n",
    "        total_samples = len(df)\n",
    "        for disease, count in disease_counts.items():\n",
    "            percentage = (count / total_samples) * 100\n",
    "            print(f\"  {disease}: {count:,} samples ({percentage:.1f}%)\")\n",
    "        print()\n",
    "        \n",
    "        # 4. Disease distribution per dataset_id\n",
    "        print(\"Disease distribution per dataset ID:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        dataset_stats = []\n",
    "        for dataset_id in sorted(dataset_ids):\n",
    "            dataset_data = df[df['dataset_id'] == dataset_id]\n",
    "            disease_dist = dataset_data['disease'].value_counts()\n",
    "            total_samples = len(dataset_data)\n",
    "            \n",
    "            print(f\"\\nDataset ID: {dataset_id}\")\n",
    "            print(f\"Total samples: {total_samples:,}\")\n",
    "            print(\"Disease distribution:\")\n",
    "            \n",
    "            dataset_stat = {'dataset_id': dataset_id, 'total_samples': total_samples}\n",
    "            for disease, count in disease_dist.items():\n",
    "                percentage = (count / total_samples) * 100\n",
    "                print(f\"  - {disease}: {count:,} samples ({percentage:.1f}%)\")\n",
    "                dataset_stat[disease] = count\n",
    "            \n",
    "            # Fill missing diseases with 0\n",
    "            for disease in disease_counts.index:\n",
    "                if disease not in dataset_stat:\n",
    "                    dataset_stat[disease] = 0\n",
    "            \n",
    "            dataset_stats.append(dataset_stat)\n",
    "        \n",
    "        # 5. Summary statistics\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Samples per dataset_id\n",
    "        samples_per_dataset = df.groupby('dataset_id').size()\n",
    "        print(f\"\\nSamples per dataset:\")\n",
    "        print(f\"  Min: {samples_per_dataset.min():,}\")\n",
    "        print(f\"  Max: {samples_per_dataset.max():,}\")\n",
    "        print(f\"  Mean: {samples_per_dataset.mean():.1f}\")\n",
    "        print(f\"  Median: {samples_per_dataset.median():.1f}\")\n",
    "        print(f\"  Std: {samples_per_dataset.std():.1f}\")\n",
    "        \n",
    "        # Show top 10 largest and smallest datasets\n",
    "        print(f\"\\nTop 10 largest datasets:\")\n",
    "        largest_datasets = samples_per_dataset.nlargest(10)\n",
    "        for dataset_id, count in largest_datasets.items():\n",
    "            print(f\"  {dataset_id}: {count:,} samples\")\n",
    "        \n",
    "        print(f\"\\nTop 10 smallest datasets:\")\n",
    "        smallest_datasets = samples_per_dataset.nsmallest(10)\n",
    "        for dataset_id, count in smallest_datasets.items():\n",
    "            print(f\"  {dataset_id}: {count:,} samples\")\n",
    "        \n",
    "        # Cross-tabulation\n",
    "        print(f\"\\nCross-tabulation (Dataset ID vs Disease):\")\n",
    "        crosstab = pd.crosstab(df['dataset_id'], df['disease'], margins=True)\n",
    "        print(\"Cross-tabulation summary (showing marginals only due to size):\")\n",
    "        print(f\"Total datasets: {len(crosstab) - 1}\")  # -1 for the 'All' row\n",
    "        print(f\"Disease totals:\")\n",
    "        for disease in disease_counts.index:\n",
    "            print(f\"  {disease}: {crosstab.loc['All', disease]:,} samples\")\n",
    "        \n",
    "        # 6. Check for any datasets with multiple diseases\n",
    "        print(f\"\\nDatasets with multiple disease types:\")\n",
    "        multi_disease_datasets = df.groupby('dataset_id')['disease'].nunique()\n",
    "        multi_disease = multi_disease_datasets[multi_disease_datasets > 1]\n",
    "        \n",
    "        if len(multi_disease) > 0:\n",
    "            print(f\"Found {len(multi_disease)} datasets with multiple disease types:\")\n",
    "            for dataset_id, num_diseases in multi_disease.items():\n",
    "                diseases = df[df['dataset_id'] == dataset_id]['disease'].unique()\n",
    "                sample_count = len(df[df['dataset_id'] == dataset_id])\n",
    "                print(f\"  {dataset_id}: {num_diseases} diseases - {list(diseases)} ({sample_count:,} samples)\")\n",
    "        else:\n",
    "            print(\"  No datasets have multiple disease types\")\n",
    "        \n",
    "        # 7. Disease distribution across datasets\n",
    "        print(f\"\\nDisease distribution across datasets:\")\n",
    "        disease_dataset_counts = df.groupby('disease')['dataset_id'].nunique()\n",
    "        for disease, dataset_count in disease_dataset_counts.items():\n",
    "            print(f\"  {disease}: appears in {dataset_count} datasets\")\n",
    "        \n",
    "        # 8. Create visualizations\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"CREATING VISUALIZATIONS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create a figure with multiple subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('CellNexus COVID Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Overall disease distribution (pie chart)\n",
    "        ax1 = axes[0, 0]\n",
    "        disease_counts.plot(kind='pie', ax=ax1, autopct='%1.1f%%', startangle=90)\n",
    "        ax1.set_title('Overall Disease Distribution', fontweight='bold')\n",
    "        ax1.set_ylabel('')\n",
    "        \n",
    "        # 2. Samples per dataset distribution (histogram)\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.hist(samples_per_dataset.values, bins=50, alpha=0.7, edgecolor='black')\n",
    "        ax2.set_title('Distribution of Sample Counts per Dataset', fontweight='bold')\n",
    "        ax2.set_xlabel('Number of Samples')\n",
    "        ax2.set_ylabel('Number of Datasets')\n",
    "        ax2.set_yscale('log')  # Log scale due to potentially wide range\n",
    "        \n",
    "        # 3. Disease distribution by dataset (stacked bar for top datasets)\n",
    "        ax3 = axes[1, 0]\n",
    "        top_datasets = samples_per_dataset.nlargest(20)\n",
    "        top_dataset_data = []\n",
    "        for dataset_id in top_datasets.index:\n",
    "            dataset_diseases = df[df['dataset_id'] == dataset_id]['disease'].value_counts()\n",
    "            row_data = {'dataset_id': dataset_id}\n",
    "            for disease in disease_counts.index:\n",
    "                row_data[disease] = dataset_diseases.get(disease, 0)\n",
    "            top_dataset_data.append(row_data)\n",
    "        \n",
    "        top_df = pd.DataFrame(top_dataset_data)\n",
    "        top_df.set_index('dataset_id', inplace=True)\n",
    "        \n",
    "        top_df.plot(kind='bar', stacked=True, ax=ax3, width=0.8)\n",
    "        ax3.set_title('Disease Distribution in Top 20 Largest Datasets', fontweight='bold')\n",
    "        ax3.set_xlabel('Dataset ID')\n",
    "        ax3.set_ylabel('Number of Samples')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.legend(title='Disease', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # 4. Box plot of sample sizes by disease\n",
    "        ax4 = axes[1, 1]\n",
    "        dataset_disease_sizes = []\n",
    "        for dataset_id in dataset_ids:\n",
    "            dataset_data = df[df['dataset_id'] == dataset_id]\n",
    "            disease = dataset_data['disease'].iloc[0]  # Since each dataset has only one disease\n",
    "            size = len(dataset_data)\n",
    "            dataset_disease_sizes.append({'disease': disease, 'size': size})\n",
    "        \n",
    "        size_df = pd.DataFrame(dataset_disease_sizes)\n",
    "        sns.boxplot(data=size_df, x='disease', y='size', ax=ax4)\n",
    "        ax4.set_title('Dataset Size Distribution by Disease', fontweight='bold')\n",
    "        ax4.set_xlabel('Disease')\n",
    "        ax4.set_ylabel('Number of Samples (log scale)')\n",
    "        ax4.set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/dataset_analysis.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Visualizations saved to: {plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # 9. Export summary statistics\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"EXPORTING SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_df = pd.DataFrame(dataset_stats)\n",
    "        summary_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/dataset_summary.csv\"\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"Dataset summary exported to: {summary_path}\")\n",
    "        \n",
    "        # Print final summary\n",
    "        print(f\"\\n📊 FINAL SUMMARY:\")\n",
    "        print(f\"  • Total samples: {len(df):,}\")\n",
    "        print(f\"  • Total datasets: {unique_datasets}\")\n",
    "        print(f\"  • Disease types: {len(disease_counts)}\")\n",
    "        print(f\"  • Diseases: {', '.join(disease_counts.index)}\")\n",
    "        print(f\"  • Average samples per dataset: {samples_per_dataset.mean():.1f}\")\n",
    "        print(f\"  • Datasets with mixed diseases: {len(multi_disease)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {str(e)}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"1. Check if the path exists\")\n",
    "    print(\"2. Verify the dataset format\")\n",
    "    print(\"3. Check column names\")\n",
    "    \n",
    "    # Try to list directory contents\n",
    "    try:\n",
    "        if os.path.exists(dataset_path):\n",
    "            contents = os.listdir(dataset_path)\n",
    "            print(f\"Directory contents: {contents}\")\n",
    "        else:\n",
    "            print(\"Dataset path does not exist\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Cannot access directory: {str(e2)}\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30570d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Read the dataset summary CSV\n",
    "csv_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/dataset_summary.csv\"\n",
    "\n",
    "print(\"Loading CellNexus COVID dataset summary...\")\n",
    "try:\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset summary loaded successfully! Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\nDataset overview:\")\n",
    "    print(f\"Total datasets: {len(df)}\")\n",
    "    print(f\"Total samples: {df['total_samples'].sum():,}\")\n",
    "    print(f\"Total COVID-19 samples: {df['COVID-19'].sum():,}\")\n",
    "    print(f\"Total normal samples: {df['normal'].sum():,}\")\n",
    "    \n",
    "    # Categorize datasets by disease composition\n",
    "    covid_only = df[(df['COVID-19'] > 0) & (df['normal'] == 0)]\n",
    "    normal_only = df[(df['normal'] > 0) & (df['COVID-19'] == 0)]\n",
    "    mixed = df[(df['COVID-19'] > 0) & (df['normal'] > 0)]\n",
    "    \n",
    "    print(f\"\\nDataset categorization:\")\n",
    "    print(f\"COVID-19 only: {len(covid_only)} datasets\")\n",
    "    print(f\"Normal only: {len(normal_only)} datasets\")\n",
    "    print(f\"Mixed: {len(mixed)} datasets\")\n",
    "    \n",
    "    # For stratified splits, we'll consider the dominant disease in each dataset\n",
    "    # For mixed datasets, we'll classify based on the majority class\n",
    "    dataset_disease_mapping = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        dataset_id = row['dataset_id']\n",
    "        covid_count = row['COVID-19']\n",
    "        normal_count = row['normal']\n",
    "        \n",
    "        if covid_count > normal_count:\n",
    "            dataset_disease_mapping[dataset_id] = 'COVID-19'\n",
    "        elif normal_count > covid_count:\n",
    "            dataset_disease_mapping[dataset_id] = 'normal'\n",
    "        else:\n",
    "            # Equal counts (rare case) - assign to COVID-19\n",
    "            dataset_disease_mapping[dataset_id] = 'COVID-19'\n",
    "    \n",
    "    # Group datasets by their dominant disease\n",
    "    disease_groups = defaultdict(list)\n",
    "    for dataset_id, disease in dataset_disease_mapping.items():\n",
    "        disease_groups[disease].append(dataset_id)\n",
    "    \n",
    "    print(f\"\\nDataset grouping by dominant disease:\")\n",
    "    for disease, datasets in disease_groups.items():\n",
    "        print(f\"{disease}: {len(datasets)} datasets\")\n",
    "        for dataset_id in datasets:\n",
    "            row = df[df['dataset_id'] == dataset_id].iloc[0]\n",
    "            print(f\"  {dataset_id}: {row['COVID-19']} COVID + {row['normal']} normal = {row['total_samples']} total\")\n",
    "    \n",
    "    # Create 5 folds with balanced disease distribution\n",
    "    def create_balanced_folds(disease_groups, n_folds=5):\n",
    "        folds = [[] for _ in range(n_folds)]\n",
    "        \n",
    "        # For each disease, distribute datasets across folds\n",
    "        for disease, datasets in disease_groups.items():\n",
    "            # Shuffle datasets within each disease group\n",
    "            shuffled_datasets = datasets.copy()\n",
    "            random.shuffle(shuffled_datasets)\n",
    "            \n",
    "            # Distribute datasets across folds in round-robin fashion\n",
    "            for i, dataset_id in enumerate(shuffled_datasets):\n",
    "                fold_idx = i % n_folds\n",
    "                folds[fold_idx].append(dataset_id)\n",
    "        \n",
    "        return folds\n",
    "    \n",
    "    # Create the folds\n",
    "    folds = create_balanced_folds(disease_groups)\n",
    "    \n",
    "    # Print fold statistics\n",
    "    print(f\"\\nFold statistics:\")\n",
    "    for i, fold in enumerate(folds):\n",
    "        fold_diseases = [dataset_disease_mapping[dataset_id] for dataset_id in fold]\n",
    "        disease_counts = {disease: fold_diseases.count(disease) for disease in ['COVID-19', 'normal']}\n",
    "        \n",
    "        # Calculate total samples in this fold\n",
    "        fold_samples = df[df['dataset_id'].isin(fold)]['total_samples'].sum()\n",
    "        fold_covid = df[df['dataset_id'].isin(fold)]['COVID-19'].sum()\n",
    "        fold_normal = df[df['dataset_id'].isin(fold)]['normal'].sum()\n",
    "        \n",
    "        print(f\"Fold {i+1}: {len(fold)} datasets\")\n",
    "        print(f\"  Dominant diseases - COVID-19: {disease_counts['COVID-19']}, Normal: {disease_counts['normal']}\")\n",
    "        print(f\"  Sample counts - COVID-19: {fold_covid:,}, Normal: {fold_normal:,}, Total: {fold_samples:,}\")\n",
    "    \n",
    "    # Create the cross-validation splits in the required format\n",
    "    cv_splits = {}\n",
    "    \n",
    "    for fold_idx in range(5):\n",
    "        # Current fold becomes validation set\n",
    "        eval_datasets = folds[fold_idx]\n",
    "        \n",
    "        # All other folds become training set\n",
    "        train_datasets = []\n",
    "        for other_fold_idx in range(5):\n",
    "            if other_fold_idx != fold_idx:\n",
    "                train_datasets.extend(folds[other_fold_idx])\n",
    "        \n",
    "        cv_splits[str(fold_idx + 1)] = {\n",
    "            \"attr_key\": \"dataset_id\",\n",
    "            \"train\": train_datasets,\n",
    "            \"eval\": eval_datasets\n",
    "        }\n",
    "    \n",
    "    # Save to JSON file\n",
    "    output_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/5fold_cv_splits.json\"\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(cv_splits, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n5-fold cross-validation splits saved to: {output_path}\")\n",
    "    \n",
    "    # Detailed analysis of each fold\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"DETAILED FOLD ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        print(f\"\\nFold {fold_num}:\")\n",
    "        \n",
    "        # Training set analysis\n",
    "        train_df = df[df['dataset_id'].isin(fold_data['train'])]\n",
    "        train_covid = train_df['COVID-19'].sum()\n",
    "        train_normal = train_df['normal'].sum()\n",
    "        train_total = train_df['total_samples'].sum()\n",
    "        \n",
    "        print(f\"  Training set:\")\n",
    "        print(f\"    Datasets: {len(fold_data['train'])}\")\n",
    "        print(f\"    COVID-19 samples: {train_covid:,} ({train_covid/train_total*100:.1f}%)\")\n",
    "        print(f\"    Normal samples: {train_normal:,} ({train_normal/train_total*100:.1f}%)\")\n",
    "        print(f\"    Total samples: {train_total:,}\")\n",
    "        \n",
    "        # Validation set analysis\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        eval_covid = eval_df['COVID-19'].sum()\n",
    "        eval_normal = eval_df['normal'].sum()\n",
    "        eval_total = eval_df['total_samples'].sum()\n",
    "        \n",
    "        print(f\"  Evaluation set:\")\n",
    "        print(f\"    Datasets: {len(fold_data['eval'])}\")\n",
    "        print(f\"    COVID-19 samples: {eval_covid:,} ({eval_covid/eval_total*100:.1f}%)\")\n",
    "        print(f\"    Normal samples: {eval_normal:,} ({eval_normal/eval_total*100:.1f}%)\")\n",
    "        print(f\"    Total samples: {eval_total:,}\")\n",
    "        \n",
    "        # List dataset IDs in eval set\n",
    "        print(f\"    Eval dataset IDs: {fold_data['eval']}\")\n",
    "    \n",
    "    # Data leakage check\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA LEAKAGE CHECK\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_train_ids = set()\n",
    "    all_eval_ids = set()\n",
    "    leakage_found = False\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        train_ids = set(fold_data['train'])\n",
    "        eval_ids = set(fold_data['eval'])\n",
    "        \n",
    "        # Check for overlap within the same fold\n",
    "        overlap = train_ids.intersection(eval_ids)\n",
    "        if overlap:\n",
    "            print(f\"❌ LEAKAGE DETECTED in Fold {fold_num}: {len(overlap)} datasets in both train and eval\")\n",
    "            print(f\"   Overlapping dataset IDs: {list(overlap)}\")\n",
    "            leakage_found = True\n",
    "        else:\n",
    "            print(f\"✅ Fold {fold_num}: No overlap between train and eval sets\")\n",
    "        \n",
    "        all_train_ids.update(train_ids)\n",
    "        all_eval_ids.update(eval_ids)\n",
    "    \n",
    "    # Check if any dataset appears in multiple eval sets\n",
    "    from collections import Counter\n",
    "    eval_counts = Counter()\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        for dataset_id in fold_data['eval']:\n",
    "            eval_counts[dataset_id] += 1\n",
    "    \n",
    "    multi_eval = {dataset_id: count for dataset_id, count in eval_counts.items() if count > 1}\n",
    "    if multi_eval:\n",
    "        print(f\"❌ LEAKAGE DETECTED: {len(multi_eval)} datasets appear in multiple eval sets\")\n",
    "        for dataset_id, count in multi_eval.items():\n",
    "            print(f\"   Dataset {dataset_id} appears in {count} eval sets\")\n",
    "        leakage_found = True\n",
    "    \n",
    "    # Verify all datasets are included exactly once in evaluation\n",
    "    all_dataset_ids = set(df['dataset_id'].tolist())\n",
    "    if all_eval_ids == all_dataset_ids:\n",
    "        print(f\"✅ All {len(all_dataset_ids)} datasets appear exactly once in evaluation sets\")\n",
    "    else:\n",
    "        missing = all_dataset_ids - all_eval_ids\n",
    "        extra = all_eval_ids - all_dataset_ids\n",
    "        if missing:\n",
    "            print(f\"❌ Missing datasets in eval sets: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"❌ Extra datasets in eval sets: {extra}\")\n",
    "        leakage_found = True\n",
    "    \n",
    "    if not leakage_found:\n",
    "        print(f\"🎉 NO DATA LEAKAGE DETECTED: All splits are properly separated!\")\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total unique datasets in training across all folds: {len(all_train_ids)}\")\n",
    "    print(f\"  Total unique datasets in evaluation across all folds: {len(all_eval_ids)}\")\n",
    "    print(f\"  Expected datasets in training per fold: {len(all_dataset_ids) * 4 // 5}\")\n",
    "    print(f\"  Expected datasets in evaluation per fold: {len(all_dataset_ids) // 5}\")\n",
    "    \n",
    "    print(f\"\\n✅ Cross-validation splits created successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Paths\n",
    "csv_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/dataset_summary.csv\"\n",
    "json_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/5fold_cv_splits.json\"\n",
    "\n",
    "print(\"Loading CellNexus COVID dataset summary and CV splits...\")\n",
    "try:\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset summary loaded successfully! Shape: {df.shape}\")\n",
    "    \n",
    "    # Load JSON splits\n",
    "    print(\"Loading cross-validation splits...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        cv_splits = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(cv_splits)} folds\")\n",
    "    \n",
    "    # COMPREHENSIVE DATA LEAKAGE CHECK\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE DATA LEAKAGE CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_train_ids = set()\n",
    "    all_eval_ids = set()\n",
    "    leakage_found = False\n",
    "    \n",
    "    # Check 1: Within-fold overlap\n",
    "    print(\"Check 1: Within-fold train/eval overlap\")\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        train_ids = set(fold_data['train'])\n",
    "        eval_ids = set(fold_data['eval'])\n",
    "        \n",
    "        overlap = train_ids.intersection(eval_ids)\n",
    "        if overlap:\n",
    "            print(f\"❌ LEAKAGE in Fold {fold_num}: {len(overlap)} datasets in both train and eval\")\n",
    "            print(f\"   Overlapping IDs: {list(overlap)}\")\n",
    "            leakage_found = True\n",
    "        else:\n",
    "            print(f\"✅ Fold {fold_num}: No train/eval overlap ({len(train_ids)} train, {len(eval_ids)} eval)\")\n",
    "        \n",
    "        all_train_ids.update(train_ids)\n",
    "        all_eval_ids.update(eval_ids)\n",
    "    \n",
    "    # Check 2: Cross-fold eval overlap\n",
    "    print(f\"\\nCheck 2: Cross-fold evaluation overlap\")\n",
    "    eval_counts = Counter()\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        for dataset_id in fold_data['eval']:\n",
    "            eval_counts[dataset_id] += 1\n",
    "    \n",
    "    multi_eval = {dataset_id: count for dataset_id, count in eval_counts.items() if count > 1}\n",
    "    if multi_eval:\n",
    "        print(f\"❌ LEAKAGE: {len(multi_eval)} datasets in multiple eval sets\")\n",
    "        for dataset_id, count in multi_eval.items():\n",
    "            print(f\"   {dataset_id}: appears in {count} eval sets\")\n",
    "        leakage_found = True\n",
    "    else:\n",
    "        print(f\"✅ No dataset appears in multiple eval sets\")\n",
    "    \n",
    "    # Check 3: Complete dataset coverage\n",
    "    print(f\"\\nCheck 3: Dataset coverage verification\")\n",
    "    all_dataset_ids = set(df['dataset_id'].tolist())\n",
    "    \n",
    "    if all_eval_ids == all_dataset_ids:\n",
    "        print(f\"✅ Perfect coverage: All {len(all_dataset_ids)} datasets appear exactly once in eval\")\n",
    "    else:\n",
    "        missing = all_dataset_ids - all_eval_ids\n",
    "        extra = all_eval_ids - all_dataset_ids\n",
    "        if missing:\n",
    "            print(f\"❌ Missing from eval: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"❌ Extra in eval: {extra}\")\n",
    "        leakage_found = True\n",
    "    \n",
    "    # Check 4: Fold size consistency\n",
    "    print(f\"\\nCheck 4: Fold size consistency\")\n",
    "    expected_eval_size = len(all_dataset_ids) // 5\n",
    "    eval_sizes = [len(fold_data['eval']) for fold_data in cv_splits.values()]\n",
    "    \n",
    "    if all(size in [expected_eval_size, expected_eval_size + 1] for size in eval_sizes):\n",
    "        print(f\"✅ Consistent fold sizes: {eval_sizes} (expected ~{expected_eval_size})\")\n",
    "    else:\n",
    "        print(f\"⚠️  Inconsistent fold sizes: {eval_sizes}\")\n",
    "    \n",
    "    # Final leakage verdict\n",
    "    print(f\"\\n{'='*20} LEAKAGE CHECK RESULT {'='*20}\")\n",
    "    if not leakage_found:\n",
    "        print(\"🎉 NO DATA LEAKAGE DETECTED - All splits are valid!\")\n",
    "    else:\n",
    "        print(\"❌ DATA LEAKAGE FOUND - Splits need to be fixed!\")\n",
    "    print(\"=\"*64)\n",
    "    \n",
    "    # CREATE COMPREHENSIVE VISUALIZATION\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"CREATING COMPREHENSIVE VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('CellNexus COVID 5-Fold Cross-Validation Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Colors for consistency\n",
    "    colors = ['#FF6B6B', '#4ECDC4']  # Red for COVID, Teal for Normal\n",
    "    disease_order = ['COVID-19', 'normal']\n",
    "    \n",
    "    # Plot 1: Sample distribution across folds\n",
    "    ax1 = axes[0, 0]\n",
    "    fold_stats = []\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        train_df = df[df['dataset_id'].isin(fold_data['train'])]\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        \n",
    "        train_covid = train_df['COVID-19'].sum()\n",
    "        train_normal = train_df['normal'].sum()\n",
    "        eval_covid = eval_df['COVID-19'].sum()\n",
    "        eval_normal = eval_df['normal'].sum()\n",
    "        \n",
    "        fold_stats.append({\n",
    "            'fold': f'Fold {fold_num}',\n",
    "            'train_covid': train_covid,\n",
    "            'train_normal': train_normal,\n",
    "            'eval_covid': eval_covid,\n",
    "            'eval_normal': eval_normal\n",
    "        })\n",
    "    \n",
    "    fold_df = pd.DataFrame(fold_stats)\n",
    "    \n",
    "    # Stacked bar for training and evaluation\n",
    "    x = np.arange(len(fold_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Training bars\n",
    "    ax1.bar(x - width/2, fold_df['train_covid'], width, label='Train COVID-19', \n",
    "           color=colors[0], alpha=0.8)\n",
    "    ax1.bar(x - width/2, fold_df['train_normal'], width, bottom=fold_df['train_covid'],\n",
    "           label='Train Normal', color=colors[1], alpha=0.8)\n",
    "    \n",
    "    # Evaluation bars\n",
    "    ax1.bar(x + width/2, fold_df['eval_covid'], width, label='Eval COVID-19', \n",
    "           color=colors[0], alpha=0.5)\n",
    "    ax1.bar(x + width/2, fold_df['eval_normal'], width, bottom=fold_df['eval_covid'],\n",
    "           label='Eval Normal', color=colors[1], alpha=0.5)\n",
    "    \n",
    "    ax1.set_title('Sample Distribution Across Folds', fontweight='bold')\n",
    "    ax1.set_xlabel('Fold')\n",
    "    ax1.set_ylabel('Number of Samples')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(fold_df['fold'])\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Dataset distribution across folds\n",
    "    ax2 = axes[0, 1]\n",
    "    dataset_counts = []\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        train_datasets = len(fold_data['train'])\n",
    "        eval_datasets = len(fold_data['eval'])\n",
    "        dataset_counts.append({\n",
    "            'fold': f'Fold {fold_num}',\n",
    "            'train': train_datasets,\n",
    "            'eval': eval_datasets\n",
    "        })\n",
    "    \n",
    "    dataset_df = pd.DataFrame(dataset_counts)\n",
    "    \n",
    "    x = np.arange(len(dataset_df))\n",
    "    ax2.bar(x - width/2, dataset_df['train'], width, label='Train Datasets', alpha=0.8)\n",
    "    ax2.bar(x + width/2, dataset_df['eval'], width, label='Eval Datasets', alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('Dataset Count Distribution Across Folds', fontweight='bold')\n",
    "    ax2.set_xlabel('Fold')\n",
    "    ax2.set_ylabel('Number of Datasets')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(dataset_df['fold'])\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Disease balance in evaluation sets\n",
    "    ax3 = axes[1, 0]\n",
    "    eval_balance = []\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        covid_samples = eval_df['COVID-19'].sum()\n",
    "        normal_samples = eval_df['normal'].sum()\n",
    "        total_samples = covid_samples + normal_samples\n",
    "        \n",
    "        eval_balance.append({\n",
    "            'fold': f'Fold {fold_num}',\n",
    "            'COVID-19': covid_samples,\n",
    "            'normal': normal_samples,\n",
    "            'covid_pct': covid_samples / total_samples * 100 if total_samples > 0 else 0,\n",
    "            'normal_pct': normal_samples / total_samples * 100 if total_samples > 0 else 0\n",
    "        })\n",
    "    \n",
    "    balance_df = pd.DataFrame(eval_balance)\n",
    "    \n",
    "    x = np.arange(len(balance_df))\n",
    "    ax3.bar(x, balance_df['COVID-19'], label='COVID-19', color=colors[0], alpha=0.8)\n",
    "    ax3.bar(x, balance_df['normal'], bottom=balance_df['COVID-19'], \n",
    "           label='Normal', color=colors[1], alpha=0.8)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (covid, normal) in enumerate(zip(covid_counts, normal_counts)):\n",
    "        total = covid + normal\n",
    "        covid_pct = covid / total * 100 if total > 0 else 0\n",
    "        normal_pct = normal / total * 100 if total > 0 else 0\n",
    "        \n",
    "        ax5.text(i, covid/2, f\"{covid_pct:.1f}%\", ha='center', va='center', \n",
    "                fontweight='bold', color='white')\n",
    "        ax5.text(i, covid + normal/2, f\"{normal_pct:.1f}%\", ha='center', va='center', \n",
    "                fontweight='bold', color='white')\n",
    "    \n",
    "    ax5.set_title('Overall Train vs Eval Distribution', fontweight='bold')\n",
    "    ax5.set_xlabel('Split Type')\n",
    "    ax5.set_ylabel('Number of Samples')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(categories)\n",
    "    ax5.legend()\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Fold balance metrics\n",
    "    ax6 = axes[2, 1]\n",
    "    \n",
    "    # Calculate balance metrics for each fold\n",
    "    balance_metrics = []\n",
    "    for i, row in balance_df.iterrows():\n",
    "        # Calculate deviation from 50-50 balance\n",
    "        covid_dev = abs(row['covid_pct'] - 50)\n",
    "        balance_score = 100 - covid_dev  # Higher is more balanced\n",
    "        balance_metrics.append(balance_score)\n",
    "    \n",
    "    bars = ax6.bar(range(len(balance_metrics)), balance_metrics, \n",
    "                   color=['green' if score > 80 else 'orange' if score > 60 else 'red' \n",
    "                         for score in balance_metrics], alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score) in enumerate(zip(bars, balance_metrics)):\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax6.set_title('Fold Balance Score (Higher = More Balanced)', fontweight='bold')\n",
    "    ax6.set_xlabel('Fold')\n",
    "    ax6.set_ylabel('Balance Score')\n",
    "    ax6.set_xticks(range(len(balance_metrics)))\n",
    "    ax6.set_xticklabels([f'Fold {i+1}' for i in range(len(balance_metrics))])\n",
    "    ax6.set_ylim(0, 110)\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add color legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='green', alpha=0.7, label='Excellent (>80)'),\n",
    "                      Patch(facecolor='orange', alpha=0.7, label='Good (60-80)'),\n",
    "                      Patch(facecolor='red', alpha=0.7, label='Poor (<60)')]\n",
    "    ax6.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/cv_splits_analysis.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Comprehensive visualization saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # DETAILED STATISTICAL ANALYSIS\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nOverall Dataset Statistics:\")\n",
    "    print(f\"  Total datasets: {len(df)}\")\n",
    "    print(f\"  Total samples: {df['total_samples'].sum():,}\")\n",
    "    print(f\"  COVID-19 samples: {df['COVID-19'].sum():,} ({df['COVID-19'].sum()/df['total_samples'].sum()*100:.1f}%)\")\n",
    "    print(f\"  Normal samples: {df['normal'].sum():,} ({df['normal'].sum()/df['total_samples'].sum()*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nFold-wise Detailed Analysis:\")\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        print(f\"\\n--- Fold {fold_num} ---\")\n",
    "        \n",
    "        # Training set\n",
    "        train_df = df[df['dataset_id'].isin(fold_data['train'])]\n",
    "        train_covid = train_df['COVID-19'].sum()\n",
    "        train_normal = train_df['normal'].sum()\n",
    "        train_total = train_covid + train_normal\n",
    "        \n",
    "        print(f\"Training Set ({len(fold_data['train'])} datasets):\")\n",
    "        print(f\"  COVID-19: {train_covid:,} samples ({train_covid/train_total*100:.1f}%)\")\n",
    "        print(f\"  Normal: {train_normal:,} samples ({train_normal/train_total*100:.1f}%)\")\n",
    "        print(f\"  Total: {train_total:,} samples\")\n",
    "        \n",
    "        # Evaluation set\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        eval_covid = eval_df['COVID-19'].sum()\n",
    "        eval_normal = eval_df['normal'].sum()\n",
    "        eval_total = eval_covid + eval_normal\n",
    "        \n",
    "        print(f\"Evaluation Set ({len(fold_data['eval'])} datasets):\")\n",
    "        print(f\"  COVID-19: {eval_covid:,} samples ({eval_covid/eval_total*100:.1f}%)\")\n",
    "        print(f\"  Normal: {eval_normal:,} samples ({eval_normal/eval_total*100:.1f}%)\")\n",
    "        print(f\"  Total: {eval_total:,} samples\")\n",
    "        \n",
    "        # Balance metrics\n",
    "        covid_balance_diff = abs((eval_covid/eval_total*100) - (train_covid/train_total*100))\n",
    "        print(f\"  Balance difference: {covid_balance_diff:.1f}% (COVID-19 % diff between train/eval)\")\n",
    "        \n",
    "        # Dataset composition\n",
    "        print(f\"  Eval dataset details:\")\n",
    "        for dataset_id in fold_data['eval']:\n",
    "            row = df[df['dataset_id'] == dataset_id].iloc[0]\n",
    "            covid_count = row['COVID-19']\n",
    "            normal_count = row['normal']\n",
    "            total_count = row['total_samples']\n",
    "            dominant = 'COVID-19' if covid_count > normal_count else 'Normal' if normal_count > covid_count else 'Mixed'\n",
    "            print(f\"    {dataset_id}: {covid_count:,} COVID + {normal_count:,} normal = {total_count:,} total ({dominant})\")\n",
    "    \n",
    "    # Cross-validation quality metrics\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"CROSS-VALIDATION QUALITY METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate variance in fold sizes\n",
    "    eval_sizes = [len(cv_splits[str(i+1)]['eval']) for i in range(5)]\n",
    "    train_sizes = [len(cv_splits[str(i+1)]['train']) for i in range(5)]\n",
    "    \n",
    "    print(f\"\\nFold Size Consistency:\")\n",
    "    print(f\"  Eval fold sizes: {eval_sizes}\")\n",
    "    print(f\"  Train fold sizes: {train_sizes}\")\n",
    "    print(f\"  Eval size variance: {np.var(eval_sizes):.2f}\")\n",
    "    print(f\"  Train size variance: {np.var(train_sizes):.2f}\")\n",
    "    \n",
    "    # Calculate balance consistency across folds\n",
    "    covid_percentages = [row['covid_pct'] for _, row in balance_df.iterrows()]\n",
    "    balance_variance = np.var(covid_percentages)\n",
    "    \n",
    "    print(f\"\\nDisease Balance Consistency:\")\n",
    "    print(f\"  COVID-19 percentages across eval folds: {[f'{p:.1f}%' for p in covid_percentages]}\")\n",
    "    print(f\"  Balance variance: {balance_variance:.2f}\")\n",
    "    print(f\"  Balance standard deviation: {np.std(covid_percentages):.1f}%\")\n",
    "    \n",
    "    # Overall quality assessment\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"OVERALL QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    quality_score = 0\n",
    "    max_score = 100\n",
    "    \n",
    "    # Leakage check (40 points)\n",
    "    if not leakage_found:\n",
    "        quality_score += 40\n",
    "        print(\"✅ No data leakage: +40 points\")\n",
    "    else:\n",
    "        print(\"❌ Data leakage detected: +0 points\")\n",
    "    \n",
    "    # Fold size consistency (20 points)\n",
    "    if np.var(eval_sizes) == 0:\n",
    "        quality_score += 20\n",
    "        print(\"✅ Perfect fold size consistency: +20 points\")\n",
    "    elif np.var(eval_sizes) <= 1:\n",
    "        quality_score += 15\n",
    "        print(\"✅ Good fold size consistency: +15 points\")\n",
    "    else:\n",
    "        quality_score += 5\n",
    "        print(\"⚠️ Poor fold size consistency: +5 points\")\n",
    "    \n",
    "    # Disease balance (25 points)\n",
    "    if balance_variance < 25:  # Less than 5% std dev\n",
    "        quality_score += 25\n",
    "        print(\"✅ Excellent disease balance: +25 points\")\n",
    "    elif balance_variance < 100:  # Less than 10% std dev\n",
    "        quality_score += 20\n",
    "        print(\"✅ Good disease balance: +20 points\")\n",
    "    elif balance_variance < 225:  # Less than 15% std dev\n",
    "        quality_score += 15\n",
    "        print(\"⚠️ Fair disease balance: +15 points\")\n",
    "    else:\n",
    "        quality_score += 5\n",
    "        print(\"⚠️ Poor disease balance: +5 points\")\n",
    "    \n",
    "    # Coverage completeness (15 points)\n",
    "    if len(all_eval_ids) == len(df):\n",
    "        quality_score += 15\n",
    "        print(\"✅ Complete dataset coverage: +15 points\")\n",
    "    else:\n",
    "        quality_score += 0\n",
    "        print(\"❌ Incomplete dataset coverage: +0 points\")\n",
    "    \n",
    "    print(f\"\\n🏆 FINAL QUALITY SCORE: {quality_score}/{max_score} ({quality_score/max_score*100:.1f}%)\")\n",
    "    \n",
    "    if quality_score >= 90:\n",
    "        print(\"🌟 EXCELLENT: Cross-validation splits are high quality!\")\n",
    "    elif quality_score >= 70:\n",
    "        print(\"👍 GOOD: Cross-validation splits are acceptable with minor issues\")\n",
    "    elif quality_score >= 50:\n",
    "        print(\"⚠️ FAIR: Cross-validation splits have some issues that should be addressed\")\n",
    "    else:\n",
    "        print(\"❌ POOR: Cross-validation splits have significant issues and should be recreated\")\n",
    "    \n",
    "    # Export detailed results\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPORTING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save detailed fold statistics\n",
    "    fold_stats_df = pd.DataFrame(fold_stats)\n",
    "    stats_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/fold_statistics.csv\"\n",
    "    fold_stats_df.to_csv(stats_path, index=False)\n",
    "    print(f\"Fold statistics exported to: {stats_path}\")\n",
    "    \n",
    "    # Save quality report\n",
    "    quality_report = {\n",
    "        'total_datasets': len(df),\n",
    "        'total_samples': int(df['total_samples'].sum()),\n",
    "        'covid_samples': int(df['COVID-19'].sum()),\n",
    "        'normal_samples': int(df['normal'].sum()),\n",
    "        'leakage_detected': leakage_found,\n",
    "        'fold_size_variance': float(np.var(eval_sizes)),\n",
    "        'balance_variance': float(balance_variance),\n",
    "        'quality_score': quality_score,\n",
    "        'covid_percentages_by_fold': covid_percentages\n",
    "    }\n",
    "    \n",
    "    report_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/quality_report.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(quality_report, f, indent=2)\n",
    "    print(f\"Quality report exported to: {report_path}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Analysis completed successfully!\")\n",
    "    print(f\"📊 Visualization: {plot_path}\")\n",
    "    print(f\"📈 Statistics: {stats_path}\")\n",
    "    print(f\"📋 Quality Report: {report_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    for i, row in balance_df.iterrows():\n",
    "        total = row['COVID-19'] + row['normal']\n",
    "        ax3.text(i, total/2, f\"{row['covid_pct']:.1f}%\", ha='center', va='center', \n",
    "                fontweight='bold', color='white')\n",
    "        ax3.text(i, row['COVID-19'] + row['normal']/2, f\"{row['normal_pct']:.1f}%\", \n",
    "                ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    ax3.set_title('Disease Balance in Evaluation Sets', fontweight='bold')\n",
    "    ax3.set_xlabel('Fold')\n",
    "    ax3.set_ylabel('Number of Samples')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(balance_df['fold'])\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Dataset size distribution in eval sets\n",
    "    ax4 = axes[1, 1]\n",
    "    all_eval_sizes = []\n",
    "    fold_labels = []\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        sizes = eval_df['total_samples'].tolist()\n",
    "        all_eval_sizes.extend(sizes)\n",
    "        fold_labels.extend([f'Fold {fold_num}'] * len(sizes))\n",
    "    \n",
    "    size_data = pd.DataFrame({'fold': fold_labels, 'size': all_eval_sizes})\n",
    "    sns.boxplot(data=size_data, x='fold', y='size', ax=ax4)\n",
    "    ax4.set_title('Dataset Size Distribution in Eval Sets', fontweight='bold')\n",
    "    ax4.set_xlabel('Fold')\n",
    "    ax4.set_ylabel('Dataset Size (log scale)')\n",
    "    ax4.set_yscale('log')\n",
    "    \n",
    "    # Plot 5: Overall statistics comparison\n",
    "    ax5 = axes[2, 0]\n",
    "    \n",
    "    # Calculate overall train/eval statistics\n",
    "    overall_stats = {\n",
    "        'train_covid': sum(fold_df['train_covid']),\n",
    "        'train_normal': sum(fold_df['train_normal']),\n",
    "        'eval_covid': sum(fold_df['eval_covid']),\n",
    "        'eval_normal': sum(fold_df['eval_normal'])\n",
    "    }\n",
    "    \n",
    "    categories = ['Train', 'Eval']\n",
    "    covid_counts = [overall_stats['train_covid'], overall_stats['eval_covid']]\n",
    "    normal_counts = [overall_stats['train_normal'], overall_stats['eval_normal']]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    ax5.bar(x, covid_counts, label='COVID-19', color=colors[0], alpha=0.8)\n",
    "    ax5.bar(x, normal_counts, bottom=covid_counts, label='Normal', color=colors[1], alpha=0.8)\n",
    "    \n",
    "    # Add percentage labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Paths - adapted for COVID dataset\n",
    "dataset_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/tokenized_30M/cellnexus_singlecell.dataset/\"\n",
    "json_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/5fold_cv_splits.json\"\n",
    "\n",
    "print(\"Loading HuggingFace COVID dataset...\")\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset = Dataset.load_from_disk(dataset_path)\n",
    "    df = dataset.to_pandas()\n",
    "    print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Load JSON splits\n",
    "    print(\"\\nLoading cross-validation splits...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        cv_splits = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(cv_splits)} folds\")\n",
    "    \n",
    "    # Check for data leakage across all folds\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHECKING FOR DATA LEAKAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_train_ids = set()\n",
    "    all_eval_ids = set()\n",
    "    leakage_found = False\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        train_ids = set(fold_data['train'])\n",
    "        eval_ids = set(fold_data['eval'])\n",
    "        \n",
    "        # Check for overlap within the same fold\n",
    "        overlap = train_ids.intersection(eval_ids)\n",
    "        if overlap:\n",
    "            print(f\"❌ LEAKAGE DETECTED in Fold {fold_num}: {len(overlap)} datasets in both train and eval\")\n",
    "            print(f\"   Overlapping IDs: {list(overlap)}\")\n",
    "            leakage_found = True\n",
    "        else:\n",
    "            print(f\"✅ Fold {fold_num}: No overlap between train and eval sets\")\n",
    "        \n",
    "        all_train_ids.update(train_ids)\n",
    "        all_eval_ids.update(eval_ids)\n",
    "    \n",
    "    # Check if any dataset appears in multiple eval sets\n",
    "    eval_counts = Counter()\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        for dataset_id in fold_data['eval']:\n",
    "            eval_counts[dataset_id] += 1\n",
    "    \n",
    "    multi_eval = {dataset_id: count for dataset_id, count in eval_counts.items() if count > 1}\n",
    "    if multi_eval:\n",
    "        print(f\"❌ LEAKAGE DETECTED: {len(multi_eval)} datasets appear in multiple eval sets\")\n",
    "        for dataset_id, count in multi_eval.items():\n",
    "            print(f\"   Dataset {dataset_id} appears in {count} eval sets\")\n",
    "        leakage_found = True\n",
    "    \n",
    "    if not leakage_found:\n",
    "        print(\"✅ NO DATA LEAKAGE DETECTED: All splits are properly separated\")\n",
    "    \n",
    "    print(f\"\\nTotal unique datasets in training across all folds: {len(all_train_ids)}\")\n",
    "    print(f\"Total unique datasets in evaluation across all folds: {len(all_eval_ids)}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create subplots: 2 rows, 3 columns (5 folds + 1 overall)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('COVID Dataset: Class Distribution Across 5-Fold Cross-Validation Splits', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Define colors for consistency - adapted for 2 diseases\n",
    "    colors = ['#FF6B6B', '#4ECDC4']  # Red for COVID-19, Teal for normal\n",
    "    disease_order = ['COVID-19', 'normal']\n",
    "    \n",
    "    # Plot each fold\n",
    "    for i, (fold_num, fold_data) in enumerate(cv_splits.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get data for this fold - using dataset_id instead of individual\n",
    "        train_df = df[df['dataset_id'].isin(fold_data['train'])]\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        \n",
    "        # Count samples by disease\n",
    "        train_counts = train_df['disease'].value_counts()\n",
    "        eval_counts = eval_df['disease'].value_counts()\n",
    "        \n",
    "        # Ensure all diseases are represented (even if count is 0)\n",
    "        train_counts = train_counts.reindex(disease_order, fill_value=0)\n",
    "        eval_counts = eval_counts.reindex(disease_order, fill_value=0)\n",
    "        \n",
    "        # Create bar plot\n",
    "        x = np.arange(len(disease_order))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, train_counts.values, width, label='Train', \n",
    "                      color=colors, alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, eval_counts.values, width, label='Eval', \n",
    "                      color=colors, alpha=0.5)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:  # Only show label if there are samples\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:  # Only show label if there are samples\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Customize subplot\n",
    "        ax.set_title(f'Fold {fold_num}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Disease Type', fontsize=10)\n",
    "        ax.set_ylabel('Number of Samples', fontsize=10)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(disease_order, fontsize=10)\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add fold statistics as text - using dataset_id terminology\n",
    "        train_total = len(train_df)\n",
    "        eval_total = len(eval_df)\n",
    "        train_datasets = len(fold_data['train'])\n",
    "        eval_datasets = len(fold_data['eval'])\n",
    "        \n",
    "        stats_text = f'Train: {train_total:,} samples ({train_datasets} datasets)\\n'\n",
    "        stats_text += f'Eval: {eval_total:,} samples ({eval_datasets} datasets)'\n",
    "        \n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=8)\n",
    "    \n",
    "    # Create overall statistics plot in the last subplot\n",
    "    ax_overall = axes[5]\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    overall_train_counts = df['disease'].value_counts().reindex(disease_order, fill_value=0)\n",
    "    \n",
    "    # Calculate average eval counts across folds\n",
    "    avg_eval_counts = []\n",
    "    for disease in disease_order:\n",
    "        total_eval = sum(df[df['dataset_id'].isin(fold_data['eval'])]['disease'].value_counts().get(disease, 0) \n",
    "                        for fold_data in cv_splits.values())\n",
    "        avg_eval_counts.append(total_eval / len(cv_splits))\n",
    "    \n",
    "    # Plot overall statistics\n",
    "    x = np.arange(len(disease_order))\n",
    "    bars1 = ax_overall.bar(x - width/2, overall_train_counts.values, width, \n",
    "                          label='Total Dataset', color=colors, alpha=0.8)\n",
    "    bars2 = ax_overall.bar(x + width/2, avg_eval_counts, width, \n",
    "                          label='Avg Eval per Fold', color=colors, alpha=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax_overall.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax_overall.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax_overall.set_title('Overall Dataset Statistics', fontsize=12, fontweight='bold')\n",
    "    ax_overall.set_xlabel('Disease Type', fontsize=10)\n",
    "    ax_overall.set_ylabel('Number of Samples', fontsize=10)\n",
    "    ax_overall.set_xticks(x)\n",
    "    ax_overall.set_xticklabels(disease_order, fontsize=10)\n",
    "    ax_overall.legend(fontsize=9)\n",
    "    ax_overall.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add overall statistics text\n",
    "    total_samples = len(df)\n",
    "    total_datasets = df['dataset_id'].nunique()\n",
    "    stats_text = f'Total: {total_samples:,} samples\\n'\n",
    "    stats_text += f'Datasets: {total_datasets}\\n'\n",
    "    stats_text += f'Avg per fold: {total_samples//5:,} samples'\n",
    "    \n",
    "    ax_overall.text(0.02, 0.98, stats_text, transform=ax_overall.transAxes, \n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                   fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/disease_classification/cellnexus_covid_disease/cv_splits_visualization.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Visualization saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED FOLD STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        print(f\"\\nFold {fold_num}:\")\n",
    "        \n",
    "        train_df = df[df['dataset_id'].isin(fold_data['train'])]\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        \n",
    "        print(f\"  Training:\")\n",
    "        print(f\"    Datasets: {len(fold_data['train'])}\")\n",
    "        print(f\"    Samples: {len(train_df):,}\")\n",
    "        train_dist = train_df['disease'].value_counts()\n",
    "        for disease in disease_order:\n",
    "            count = train_dist.get(disease, 0)\n",
    "            pct = (count / len(train_df)) * 100 if len(train_df) > 0 else 0\n",
    "            print(f\"    {disease}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"  Evaluation:\")\n",
    "        print(f\"    Datasets: {len(fold_data['eval'])}\")\n",
    "        print(f\"    Samples: {len(eval_df):,}\")\n",
    "        eval_dist = eval_df['disease'].value_counts()\n",
    "        for disease in disease_order:\n",
    "            count = eval_dist.get(disease, 0)\n",
    "            pct = (count / len(eval_df)) * 100 if len(eval_df) > 0 else 0\n",
    "            print(f\"    {disease}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Show dataset composition in eval set\n",
    "        print(f\"  Evaluation dataset breakdown:\")\n",
    "        for dataset_id in sorted(fold_data['eval']):\n",
    "            dataset_samples = df[df['dataset_id'] == dataset_id]\n",
    "            dataset_dist = dataset_samples['disease'].value_counts()\n",
    "            total_dataset_samples = len(dataset_samples)\n",
    "            covid_count = dataset_dist.get('COVID-19', 0)\n",
    "            normal_count = dataset_dist.get('normal', 0)\n",
    "            dominant = 'COVID-19' if covid_count > normal_count else 'normal' if normal_count > covid_count else 'mixed'\n",
    "            print(f\"    {dataset_id}: {total_dataset_samples:,} samples ({covid_count:,} COVID, {normal_count:,} normal) - {dominant}\")\n",
    "    \n",
    "    # Additional COVID-specific analysis\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"COVID-SPECIFIC ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze dataset types across folds\n",
    "    print(f\"\\nDataset type distribution across folds:\")\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        eval_datasets = fold_data['eval']\n",
    "        covid_only = 0\n",
    "        normal_only = 0\n",
    "        mixed = 0\n",
    "        \n",
    "        for dataset_id in eval_datasets:\n",
    "            dataset_samples = df[df['dataset_id'] == dataset_id]\n",
    "            covid_count = len(dataset_samples[dataset_samples['disease'] == 'COVID-19'])\n",
    "            normal_count = len(dataset_samples[dataset_samples['disease'] == 'normal'])\n",
    "            \n",
    "            if covid_count > 0 and normal_count == 0:\n",
    "                covid_only += 1\n",
    "            elif normal_count > 0 and covid_count == 0:\n",
    "                normal_only += 1\n",
    "            elif covid_count > 0 and normal_count > 0:\n",
    "                mixed += 1\n",
    "        \n",
    "        print(f\"  Fold {fold_num}: {covid_only} COVID-only, {normal_only} normal-only, {mixed} mixed datasets\")\n",
    "    \n",
    "    # Disease balance analysis\n",
    "    print(f\"\\nDisease balance across evaluation folds:\")\n",
    "    for fold_num, fold_data in cv_splits.items():\n",
    "        eval_df = df[df['dataset_id'].isin(fold_data['eval'])]\n",
    "        covid_samples = len(eval_df[eval_df['disease'] == 'COVID-19'])\n",
    "        normal_samples = len(eval_df[eval_df['disease'] == 'normal'])\n",
    "        total_samples = covid_samples + normal_samples\n",
    "        \n",
    "        covid_pct = (covid_samples / total_samples * 100) if total_samples > 0 else 0\n",
    "        normal_pct = (normal_samples / total_samples * 100) if total_samples > 0 else 0\n",
    "        balance_score = 100 - abs(covid_pct - 50)  # Higher score = more balanced\n",
    "        \n",
    "        print(f\"  Fold {fold_num}: {covid_pct:.1f}% COVID-19, {normal_pct:.1f}% normal (Balance score: {balance_score:.1f})\")\n",
    "    \n",
    "    print(f\"\\n✅ Analysis completed successfully!\")\n",
    "    print(f\"📊 Visualization saved to: {plot_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e72f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
