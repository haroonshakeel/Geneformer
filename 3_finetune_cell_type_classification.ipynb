{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from geneformer import Classifier\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://p2-gpu-1:8144/lab?token=61cf43513f8b71cd83fa2309252ddcd14bf34e05457c22d7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccac4e-9b20-4f28-9ec7-a31aba158128",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec1953-38a6-4051-82bc-00247ecf13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 0\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "seed_val = 42\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "BASE_DIR = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer\"\n",
    "AGGREGATION_LEVEL = \"metacell_8\"  # Set aggregation level to 'singlecell' for single-cell data\n",
    "MODEL_VARIANT = \"30M\"  # Specify the model variant, e.g., \"30M\" or \"95M\"\n",
    "TASK = \"cell_type_classification\"  # Specify the task, e.g., \"disease_classification\" or \"dosage_sensitivity\"\n",
    "DATASET = \"cellnexus_cell_types\"\n",
    "\n",
    "VALID_COMBINATIONS = {\n",
    "    \"disease_classification\": [\"genecorpus_heart_disease\", \"cellnexus_blood_disease\"],\n",
    "    \"dosage_sensitivity\": [\"genecorpus_dosage_sensitivity\"],\n",
    "    \"cell_type_classification\": [\"cellnexus_cell_types\"]\n",
    "}\n",
    "\n",
    "assert TASK in VALID_COMBINATIONS, f\"Unknown TASK: '{TASK}'\"\n",
    "assert DATASET in VALID_COMBINATIONS[TASK], \\\n",
    "    f\"For TASK='{TASK}', DATASET must be one of {VALID_COMBINATIONS[TASK]}, got '{DATASET}'\"\n",
    "                         \n",
    "if MODEL_VARIANT == \"30M\":\n",
    "    GENE_MEDIAN_FILE = os.path.join(BASE_DIR, \"Geneformer/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl\")\n",
    "    TOKEN_DICTIONARY_FILE = os.path.join(BASE_DIR, \"Geneformer/geneformer/gene_dictionaries_30m/token_dictionary_gc30M.pkl\")\n",
    "    ENSEMBL_MAPPING_FILE = os.path.join(BASE_DIR, \"Geneformer/geneformer/gene_dictionaries_30m/ensembl_mapping_dict_gc30M.pkl\")\n",
    "elif MODEL_VARIANT == \"95M\":\n",
    "    GENE_MEDIAN_FILE = os.path.join(BASE_DIR, \"Geneformer/geneformer/gene_median_dictionary_gc95M.pkl\")\n",
    "    TOKEN_DICTIONARY_FILE = os.path.join(BASE_DIR, \"Geneformer/geneformer/token_dictionary_gc95M.pkl\")\n",
    "    ENSEMBL_MAPPING_FILE = os.path.join(BASE_DIR, \"Geneformer/geneformer/ensembl_mapping_dict_gc95M.pkl\")\n",
    "else:\n",
    "    raise ValueError(\"MODEL_VARIANT must be either '30M' or '95M'\")\n",
    "\n",
    "# Print the final paths to verify\n",
    "print(f\"GENE_MEDIAN_FILE: {GENE_MEDIAN_FILE}\")\n",
    "print(f\"TOKEN_DICTIONARY_FILE: {TOKEN_DICTIONARY_FILE}\")\n",
    "print(f\"ENSEMBL_MAPPING_FILE: {ENSEMBL_MAPPING_FILE}\")\n",
    "\n",
    "\n",
    "if TASK == \"disease_classification\" or TASK == \"cell_type_classification\":\n",
    "    classifier_type = \"cell\"\n",
    "elif TASK == \"dosage_sensitivity\":\n",
    "    classifier_type = \"gene\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"TASK must be either 'disease_classification' or 'dosage_sensitivity'\")\n",
    "\n",
    "\n",
    "model_version = \"V1\" # For now it does not do anything, but it is here for future compatibility\n",
    "\n",
    " # model_version : str\n",
    " #            | To auto-select settings for model version other than current default.\n",
    " #            | Current options: V1: models pretrained on ~30M cells, V2: models pretrained on ~104M cells   \n",
    "\n",
    "output_dir = os.path.join(BASE_DIR, \"trained_cell_classification_models\", TASK, DATASET, str(MODEL_VARIANT) + \"_\" + str(AGGREGATION_LEVEL))\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(BASE_DIR, \"datasets\", TASK, DATASET)\n",
    "\n",
    "DATASET_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa431b66-9c0d-4af4-a7b7-ce2f31947c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"disease_classification\" and DATASET == \"genecorpus_heart_disease\":\n",
    "    input_data_file = os.path.join(DATASET_PATH, \"human_dcm_hcm_nf.dataset\")\n",
    "    cell_state_dict = {\"state_key\": \"disease\", \"states\": \"all\"}\n",
    "    filter_data_dict={\"cell_type\":[\"Cardiomyocyte1\",\"Cardiomyocyte2\",\"Cardiomyocyte3\"]}\n",
    "\n",
    "    # previously balanced splits with prepare_data and validate functions\n",
    "    # argument attr_to_split set to \"individual\" and attr_to_balance set to [\"disease\",\"lvef\",\"age\",\"sex\",\"length\"]\n",
    "    train_ids = [\"1447\", \"1600\", \"1462\", \"1558\", \"1300\", \"1508\", \"1358\", \"1678\", \"1561\", \"1304\", \"1610\", \"1430\", \"1472\", \"1707\", \"1726\", \"1504\", \"1425\", \"1617\", \"1631\", \"1735\", \"1582\", \"1722\", \"1622\", \"1630\", \"1290\", \"1479\", \"1371\", \"1549\", \"1515\"]\n",
    "    eval_ids = [\"1422\", \"1510\", \"1539\", \"1606\", \"1702\"]\n",
    "    test_ids = [\"1437\", \"1516\", \"1602\", \"1685\", \"1718\"]\n",
    "    \n",
    "    train_test_id_split_dict = {\"attr_key\": \"individual\",\n",
    "                                \"train\": train_ids+eval_ids,\n",
    "                                \"test\": test_ids}\n",
    "\n",
    "    train_valid_id_split_dict = {\"attr_key\": \"individual\",\n",
    "                            \"train\": train_ids,\n",
    "                            \"eval\": eval_ids}\n",
    "elif TASK == \"disease_classification\" and DATASET == \"cellnexus_blood_disease\":\n",
    "    input_data_file = os.path.join(DATASET_PATH, \"tokenized_\" + str(MODEL_VARIANT), \"cellnexus_singlecell.dataset\")\n",
    "    import json\n",
    "    filter_data_dict = None\n",
    "    cell_state_dict = {\"state_key\": \"disease\", \"states\": \"all\"}\n",
    "\n",
    "\n",
    "    # Load train_test split dictionary\n",
    "    train_test_file = os.path.join(DATASET_PATH, \"train_test_id_split_dict.json\")\n",
    "    print(f\"Loading train_test split from: {train_test_file}\")\n",
    "\n",
    "    with open(train_test_file, 'r') as f:\n",
    "        train_test_id_split_dict = json.load(f)\n",
    "\n",
    "    # Load train_valid split dictionary  \n",
    "    train_valid_file = os.path.join(DATASET_PATH, \"train_valid_id_split_dict.json\")\n",
    "    print(f\"Loading train_valid split from: {train_valid_file}\")\n",
    "\n",
    "    with open(train_valid_file, 'r') as f:\n",
    "        train_valid_id_split_dict = json.load(f)\n",
    "\n",
    "    # Verify the loaded dictionaries\n",
    "    print(\"\\n=== Train-Test Split Dictionary ===\")\n",
    "    print(f\"Attribute key: {train_test_id_split_dict['attr_key']}\")\n",
    "    print(f\"Train samples: {len(train_test_id_split_dict['train'])}\")\n",
    "    print(f\"Test samples: {len(train_test_id_split_dict['test'])}\")\n",
    "\n",
    "    print(\"\\n=== Train-Valid Split Dictionary ===\")\n",
    "    print(f\"Attribute key: {train_valid_id_split_dict['attr_key']}\")\n",
    "    print(f\"Train samples: {len(train_valid_id_split_dict['train'])}\")\n",
    "    print(f\"Eval samples: {len(train_valid_id_split_dict['eval'])}\")\n",
    "\n",
    "    # Show a few sample IDs for verification\n",
    "    print(f\"\\nSample train IDs: {train_test_id_split_dict['train'][:5]}\")\n",
    "    print(f\"Sample test IDs: {train_test_id_split_dict['test'][:5]}\")\n",
    "    print(f\"Sample eval IDs: {train_valid_id_split_dict['eval'][:5]}\")\n",
    "\n",
    "    print(\"\\n✅ Dictionaries loaded successfully!\")\n",
    "        # previously balanced splits with prepare_data and validate functions\n",
    "        # argument attr_to_split set to \"individual\" and attr_to_balance set to [\"disease\",\"\n",
    "elif TASK == \"cell_type_classification\" and DATASET == \"cellnexus_cell_types\":\n",
    "    input_data_file = os.path.join(DATASET_PATH, \"tokenized_\" + str(MODEL_VARIANT), \"cellnexus_singlecell.dataset\")\n",
    "    cell_state_dict = {\"state_key\": \"cell_type\", \"states\": \"all\"}\n",
    "    filter_data_dict = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba55ca-ec87-4c64-9fb3-3d2b01588d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641a426-3ff7-479d-9d03-2ed3c54acdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_prefix = DATASET + \"_test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fce018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = {\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"learning_rate\": 0.000804,\n",
    "    \"lr_scheduler_type\": \"polynomial\",\n",
    "    \"warmup_steps\": 1812,\n",
    "    \"weight_decay\":0.258828,\n",
    "    \"per_device_train_batch_size\": 128,\n",
    "    \"seed\": 73,\n",
    "    \n",
    "    \n",
    "    # \"logging_dir\": os.path.normpath(\"D:/geneformer_finetuning/trained_cell_classification_models/disease_classification/genecorpus_heart_disease/30M_metacell_8/250623_geneformer_cellClassifier_genecorpus_heart_disease_test/ksplit1/runs\"),\n",
    "}\n",
    "\n",
    "# OF NOTE: token_dictionary_file must be set to the gc-30M token dictionary if using a 30M series model\n",
    "# (otherwise the Classifier will use the current default model dictionary)\n",
    "# 30M token dictionary: https://huggingface.co/ctheodoris/Geneformer/blob/main/geneformer/gene_dictionaries_30m/token_dictionary_gc30M.pkl\n",
    "if TASK == \"disease_classification\":\n",
    "    cc = Classifier(classifier=classifier_type,\n",
    "                    # model_version = model_version,\n",
    "                    cell_state_dict = cell_state_dict,\n",
    "                    filter_data=filter_data_dict,\n",
    "                    training_args=training_args,\n",
    "                    max_ncells=None,\n",
    "                    freeze_layers = 2,\n",
    "                    num_crossval_splits = 1,\n",
    "                    ngpu =  torch.cuda.device_count(),\n",
    "                    forward_batch_size=200,\n",
    "                    token_dictionary_file = TOKEN_DICTIONARY_FILE,\n",
    "                    nproc=16)\n",
    "                    \n",
    "elif TASK == \"cell_type_classification\":\n",
    "    cc = Classifier(classifier=classifier_type,\n",
    "                    # model_version = model_version,\n",
    "                    cell_state_dict = cell_state_dict,\n",
    "                    filter_data=filter_data_dict,\n",
    "                    training_args=training_args,\n",
    "                    max_ncells=None,\n",
    "                    freeze_layers = 4,\n",
    "                    num_crossval_splits = 1,\n",
    "                    split_sizes = {\"train\": 0.8, \"valid\": 0.1, \"test\": 0.1},\n",
    "                    stratify_splits_col = \"cell_type_backup\", #cell_type is (from cell_state_dict) internall renamed to label\n",
    "                    ngpu =  torch.cuda.device_count(),\n",
    "                    forward_batch_size=200,\n",
    "                    token_dictionary_file = TOKEN_DICTIONARY_FILE,\n",
    "                    nproc=16)\n",
    "\n",
    "### num_crossval_splits : {0, 1, 5}\n",
    "        #     | 0: train on all data without splitting\n",
    "        #     | 1: split data into train and eval sets by designated split_sizes[\"valid\"]\n",
    "        #     | 5: split data into 5 folds of train and eval sets by designated split_sizes[\"valid\"]\n",
    "        # split_sizes : None, dict\n",
    "        #     | Dictionary of proportion of data to hold out for train, validation, and test sets\n",
    "        #     | {\"train\": 0.8, \"valid\": 0.1, \"test\": 0.1} if intending 80/10/10 train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc73101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535926f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b48148",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def check_and_copy_datasets(output_dir):\n",
    "    \"\"\"\n",
    "    Check if dataset paths exist and copy them to output directory\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Destination directory to copy datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the dataset paths\n",
    "    base_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_cell_classification_models/cell_type_classification/cellnexus_cell_types/30M_metacell_16/\"\n",
    "    \n",
    "    train_dataset_path = os.path.join(base_path, \"cellnexus_cell_types_test_labeled_train.dataset\")\n",
    "    test_dataset_path = os.path.join(base_path, \"cellnexus_cell_types_test_labeled_test.dataset\")\n",
    "    ids_path = os.path.join(base_path, \"cellnexus_cell_types_test_id_class_dict.pkl\")\n",
    "    \n",
    "    paths_to_check = {\n",
    "        \"train\": train_dataset_path,\n",
    "        \"test\": test_dataset_path,\n",
    "        \"ids\": ids_path\n",
    "    }\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Check if paths exist and copy them\n",
    "    for dataset_type, path in paths_to_check.items():\n",
    "        print(f\"\\nChecking {dataset_type} dataset...\")\n",
    "        print(f\"Path: {path}\")\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            print(f\"✅ {dataset_type.capitalize()} dataset EXISTS\")\n",
    "            \n",
    "            # Define destination path\n",
    "            dataset_name = os.path.basename(path)\n",
    "            destination = os.path.join(output_dir, dataset_name)\n",
    "            \n",
    "            try:\n",
    "                print(f\"Copying to: {destination}\")\n",
    "                \n",
    "                # Copy the dataset (use copytree for directories, copy2 for files)\n",
    "                if os.path.isdir(path):\n",
    "                    if os.path.exists(destination):\n",
    "                        print(f\"Destination already exists, skipping: {destination}\")\n",
    "                    #     shutil.rmtree(destination)\n",
    "                    else:\n",
    "                        shutil.copytree(path, destination)\n",
    "                else:\n",
    "                    if os.path.exists(destination):\n",
    "                        print(f\"Destination already exists, skipping: {destination}\")\n",
    "                    else:\n",
    "                        shutil.copy2(path, destination)\n",
    "                \n",
    "                print(f\"✅ Successfully copied {dataset_type} dataset\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error copying {dataset_type} dataset: {str(e)}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"❌ {dataset_type.capitalize()} dataset DOES NOT EXIST\")\n",
    "    \n",
    "    print(f\"\\n📁 Contents of output directory '{output_dir}':\")\n",
    "    try:\n",
    "        for item in os.listdir(output_dir):\n",
    "            item_path = os.path.join(output_dir, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"  📂 {item}/\")\n",
    "            else:\n",
    "                print(f\"  📄 {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing output directory: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830338ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "# Example input_data_file for 30M model: https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/example_input_files/cell_classification/disease_classification/human_dcm_hcm_nf.dataset\n",
    "if TASK == \"disease_classification\":\n",
    "    cc.prepare_data(input_data_file=input_data_file,\n",
    "                    output_directory=output_dir,\n",
    "                    output_prefix=output_prefix,\n",
    "                    split_id_dict=train_test_id_split_dict)\n",
    "\n",
    "elif TASK == \"cell_type_classification\":\n",
    "\n",
    "    # Define the dataset paths\n",
    "    base_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_cell_classification_models/cell_type_classification/cellnexus_cell_types/30M_metacell_16/\"\n",
    "    \n",
    "    train_dataset_path = os.path.join(base_path, \"cellnexus_cell_types_test_labeled_train.dataset\")\n",
    "    test_dataset_path = os.path.join(base_path, \"cellnexus_cell_types_test_labeled_test.dataset\")\n",
    "        \n",
    "    if os.path.exists(train_dataset_path) and os.path.exists(test_dataset_path):\n",
    "\n",
    "        print(\"Existing data for cell type classification. Skipping preparation...\")        \n",
    "        \n",
    "        check_and_copy_datasets(output_dir)\n",
    "    else:\n",
    "        print(\"Preparing data for cell type classification...\")\n",
    "        cc.prepare_data(input_data_file=input_data_file,\n",
    "                        output_directory=output_dir,\n",
    "                        output_prefix=output_prefix,\n",
    "                        test_size=0.1,\n",
    "                        split_id_dict=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb91b96-b008-498e-85cb-5008168e582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Base path\n",
    "pretrained_model_path = os.path.join(BASE_DIR, \"trained_foundation_models\", \"models\", \n",
    "                                   f\"30M_AGG{AGGREGATION_LEVEL}_6_emb256_SL2048_E2_B12_LR0.001_LSlinear_WU10000_Oadamw\")\n",
    "\n",
    "# Check if final_trained_model subfolder exists\n",
    "final_model_path = os.path.join(pretrained_model_path, \"final_trained_model\", AGGREGATION_LEVEL)\n",
    "final_model_path_empty = False\n",
    "\n",
    "if os.path.exists(final_model_path) and os.path.isdir(final_model_path):\n",
    "    # Check if the folder has model files (not just empty)\n",
    "    model_files = glob.glob(os.path.join(final_model_path, \"*.bin\")) + \\\n",
    "                  glob.glob(os.path.join(final_model_path, \"*.safetensors\")) + \\\n",
    "                  glob.glob(os.path.join(final_model_path, \"config.json\"))\n",
    "    \n",
    "    if model_files:\n",
    "        pretrained_model_path = final_model_path\n",
    "        print(f\"✓ Using final trained model: {pretrained_model_path}\")\n",
    "    else:\n",
    "        print(f\"⚠ Final model folder exists but appears empty: {final_model_path}\")\n",
    "        final_model_path_empty = True\n",
    "        # Fall through to checkpoint search\n",
    "else:\n",
    "    print(f\"ℹ Final model folder not found: {final_model_path}\")\n",
    "\n",
    "# If final model not found or empty, find most recent checkpoint\n",
    "if not (os.path.exists(final_model_path)) or final_model_path_empty:\n",
    "    # Find all checkpoint folders\n",
    "    checkpoint_pattern = os.path.join(pretrained_model_path, \"checkpoint-*\")\n",
    "    checkpoint_folders = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if checkpoint_folders:\n",
    "        # Extract checkpoint numbers and find the highest one\n",
    "        checkpoint_numbers = []\n",
    "        for folder in checkpoint_folders:\n",
    "            folder_name = os.path.basename(folder)\n",
    "            if folder_name.startswith(\"checkpoint-\"):\n",
    "                try:\n",
    "                    checkpoint_num = int(folder_name.split(\"-\")[1])\n",
    "                    checkpoint_numbers.append((checkpoint_num, folder))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        if checkpoint_numbers:\n",
    "            # Sort by checkpoint number and get the highest one\n",
    "            most_recent_checkpoint = max(checkpoint_numbers, key=lambda x: x[0])\n",
    "            pretrained_model_path = most_recent_checkpoint[1]\n",
    "            print(f\"✓ Using most recent checkpoint: {pretrained_model_path} (step {most_recent_checkpoint[0]})\")\n",
    "        else:\n",
    "            print(f\"❌ No valid checkpoint folders found in: {pretrained_model_path}\")\n",
    "            raise FileNotFoundError(f\"No trained model or checkpoints found in {pretrained_model_path}\")\n",
    "    else:\n",
    "        print(f\"❌ No checkpoint folders found in: {pretrained_model_path}\")\n",
    "        raise FileNotFoundError(f\"No trained model or checkpoints found in {pretrained_model_path}\")\n",
    "\n",
    "print(f\"Final pretrained model path: {pretrained_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec606b-96cf-4b7f-9cd9-b34a6556db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6fe4fa-db01-4085-aadc-d1af3aa46a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{output_dir}/{output_prefix}_labeled_train.dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2f4ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if TASK == \"disease_classification\":\n",
    "\n",
    "    # Example 6 layer 30M Geneformer model: https://huggingface.co/ctheodoris/Geneformer/blob/main/gf-6L-30M-i2048/model.safetensors\n",
    "    all_metrics = cc.validate(model_directory=pretrained_model_path,\n",
    "                            prepared_input_data_file=f\"{output_dir}/{output_prefix}_labeled_train.dataset\",\n",
    "                            id_class_dict_file=f\"{output_dir}/{output_prefix}_id_class_dict.pkl\",\n",
    "                            output_directory=output_dir,\n",
    "                            output_prefix=output_prefix,\n",
    "                            split_id_dict=train_valid_id_split_dict)\n",
    "                            # to optimize hyperparameters, set n_hyperopt_trials=100 (or alternative desired # of trials)\n",
    "\n",
    "elif TASK == \"cell_type_classification\":\n",
    "    all_metrics = cc.validate(model_directory=pretrained_model_path,\n",
    "                            prepared_input_data_file=f\"{output_dir}/{output_prefix}_labeled_train.dataset\",\n",
    "                            id_class_dict_file=f\"{output_dir}/{output_prefix}_id_class_dict.pkl\",\n",
    "                            output_directory=output_dir,\n",
    "                            output_prefix=output_prefix,\n",
    "                            split_id_dict=None)\n",
    "                            # to optimize hyperparameters, set n_hyperopt_trials=100 (or alternative desired # of trials)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Classifier(classifier=classifier_type,\n",
    "                cell_state_dict = cell_state_dict,\n",
    "                forward_batch_size=200,\n",
    "                token_dictionary_file = TOKEN_DICTIONARY_FILE,\n",
    "                nproc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c87e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_date = \"250625\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4619bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_test = cc.evaluate_saved_model(\n",
    "        model_directory=f\"{output_dir}/{training_date}_geneformer_cellClassifier_{output_prefix}/ksplit1/\",\n",
    "        id_class_dict_file=f\"{output_dir}/{output_prefix}_id_class_dict.pkl\",\n",
    "        test_data_file=f\"{output_dir}/{output_prefix}_labeled_test.dataset\",\n",
    "        output_directory=output_dir,\n",
    "        output_prefix=output_prefix,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def plot_conf_mat(\n",
    "        conf_mat_dict,\n",
    "        output_directory,\n",
    "        output_prefix,\n",
    "        custom_class_order=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix results of evaluating the fine-tuned model.\n",
    "\n",
    "        **Parameters**\n",
    "\n",
    "        conf_mat_dict : dict\n",
    "            | Dictionary of model_name : confusion_matrix_DataFrame\n",
    "            | (all_metrics[\"conf_matrix\"] from self.validate)\n",
    "        output_directory : Path\n",
    "            | Path to directory where plots will be saved\n",
    "        output_prefix : str\n",
    "            | Prefix for output file\n",
    "        custom_class_order : None, list\n",
    "            | List of classes in custom order for plots.\n",
    "            | Same order will be used for all models.\n",
    "        \"\"\"\n",
    "\n",
    "        for model_name in conf_mat_dict.keys():\n",
    "            plot_confusion_matrix(\n",
    "                conf_mat_dict[model_name],\n",
    "                model_name,\n",
    "                output_directory,\n",
    "                output_prefix,\n",
    "                custom_class_order,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "    conf_mat_df, title, output_dir, output_prefix, custom_class_order\n",
    "):\n",
    "    # fig = plt.figure()\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    # sns.set(font_scale=1)\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(20, 16)  # Much larger figure\n",
    "    sns.set(font_scale=0.8)  \n",
    "    sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "    if custom_class_order is not None:\n",
    "        conf_mat_df = conf_mat_df.reindex(\n",
    "            index=custom_class_order, columns=custom_class_order\n",
    "        )\n",
    "    display_labels = generate_display_labels(conf_mat_df)\n",
    "    conf_mat = preprocessing.normalize(conf_mat_df.to_numpy(), norm=\"l1\")\n",
    "    display = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=conf_mat, display_labels=display_labels\n",
    "    )\n",
    "    display.plot(cmap=\"Blues\", values_format=\".2f\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    plt.xticks(rotation=90, ha='center', fontsize=10)  # Vertical rotation, smaller font\n",
    "    plt.yticks(rotation=0, fontsize=10)                # Smaller font for y-axis\n",
    "    plt.tight_layout()                                 # Better spacing\n",
    "    # Rotate x-axis labels to prevent overlap\n",
    "    plt.yticks(rotation=0)\n",
    "    # plt.title(title)\n",
    "    plt.title(f\"Cell Type ({AGGREGATION_LEVEL})\", fontsize=16, pad=20)\n",
    "    \n",
    "\n",
    "    output_file = (Path(output_dir) / f\"{output_prefix}_conf_mat\").with_suffix(\".pdf\")\n",
    "    display.figure_.savefig(output_file, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_display_labels(conf_mat_df):\n",
    "    display_labels = []\n",
    "    i = 0\n",
    "    for label in conf_mat_df.index:\n",
    "        display_labels += [f\"{label}\\nn={conf_mat_df.iloc[i,:].sum():.0f}\"]\n",
    "        i = i + 1\n",
    "    return display_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce59fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3046187",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conf_mat(\n",
    "        conf_mat_dict={\"Geneformer\": all_metrics_test[\"conf_matrix\"]},\n",
    "        output_directory=output_dir,\n",
    "        output_prefix=output_prefix,\n",
    "        # custom_class_order=[\"nf\",\"hcm\",\"dcm\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.plot_predictions(\n",
    "#     predictions_file=f\"{output_dir}/{output_prefix}_pred_dict.pkl\",\n",
    "#     id_class_dict_file=f\"{output_dir}/{output_prefix}_id_class_dict.pkl\",\n",
    "#     title=\"cell_type\" if  TASK == \"cell_type_classification\" else \"disease\",\n",
    "#     output_directory=output_dir,\n",
    "#     output_prefix=output_prefix,\n",
    "#     # custom_class_order=[\"nf\",\"hcm\",\"dcm\"],\n",
    "#     # [\"Non-failing\",\"Hypertrophic\",\"Dilated Cardiomyopathy\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcec585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(all_metrics_test, open(Path(output_dir) / f\"{output_prefix}_all_metrics_test.json\", 'w'), indent=2, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1ab21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436240c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d20ff0",
   "metadata": {},
   "source": [
    "# Plot Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer import EmbExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(output_dir, \"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77146b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(output_dir, \"embeddings\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate EmbExtractor\n",
    "# OF NOTE: token_dictionary_file must be set to the gc-30M token dictionary if using a 30M series model\n",
    "# (otherwise the EmbExtractor will use the current default model dictionary)\n",
    "embex = EmbExtractor(model_type=\"CellClassifier\",\n",
    "                     num_classes=9 if DATASET == \"cellnexus_blood_disease\" else 51, # 9 classes for disease classification\n",
    "                     filter_data=filter_data_dict,\n",
    "                     max_ncells= 100000, # 10000 if DATASET == \"cellnexus_blood_disease\" else 1000,\n",
    "                     emb_layer=0, # | Embedding layer to extract.\n",
    "            # | The last layer is most specifically weighted to optimize the given learning objective.\n",
    "            # | Generally, it is best to extract the 2nd to last layer to get a more general representation.\n",
    "            # | -1: 2nd to last layer\n",
    "            # | 0: last layer\n",
    "                     emb_label=[\"cell_type\"],\n",
    "                     labels_to_plot=[\"cell_type\"],\n",
    "                     forward_batch_size=200,\n",
    "                     nproc=16,\n",
    "                     emb_mode = \"cell\",\n",
    "                     token_dictionary_file=TOKEN_DICTIONARY_FILE) # change from current default dictionary for 30M model series\n",
    "\n",
    "# extracts embedding from input data\n",
    "# input data is tokenized rank value encodings generated by Geneformer tokenizer (see tokenizing_scRNAseq_data.ipynb)\n",
    "# example dataset for 30M model series: https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/example_input_files/cell_classification/disease_classification/human_dcm_hcm_nf.dataset\n",
    "embs = embex.extract_embs(model_directory=f\"{output_dir}/{training_date}_geneformer_cellClassifier_{output_prefix}/ksplit1/\", # example 30M fine-tuned model\n",
    "                          input_data_file=input_data_file,\n",
    "                          output_directory=os.path.join(output_dir, \"embeddings\"),\n",
    "                          output_prefix=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7395fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_kwargs_dict = {\"size\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def gen_heatmap_class_dict(classes, label_colors_series):\n",
    "    class_color_dict_df = pd.DataFrame(\n",
    "        {\"classes\": classes, \"color\": label_colors_series}\n",
    "    )\n",
    "    class_color_dict_df = class_color_dict_df.drop_duplicates(subset=[\"classes\"])\n",
    "    return dict(zip(class_color_dict_df[\"classes\"], class_color_dict_df[\"color\"]))\n",
    "\n",
    "def gen_heatmap_class_colors(labels, df):\n",
    "    pal = sns.cubehelix_palette(\n",
    "        len(Counter(labels).keys()),\n",
    "        light=0.9,\n",
    "        dark=0.1,\n",
    "        hue=1,\n",
    "        reverse=True,\n",
    "        start=1,\n",
    "        rot=-2,\n",
    "    )\n",
    "    lut = dict(zip(map(str, Counter(labels).keys()), pal))\n",
    "    colors = pd.Series(labels, index=df.index).map(lut)\n",
    "    return colors\n",
    "\n",
    "def make_colorbar(embs_df, label):\n",
    "    labels = list(embs_df[label])\n",
    "\n",
    "    cell_type_colors = gen_heatmap_class_colors(labels, embs_df)\n",
    "    label_colors = pd.DataFrame(cell_type_colors, columns=[label])\n",
    "\n",
    "    # create dictionary for colors and classes\n",
    "    label_color_dict = gen_heatmap_class_dict(labels, label_colors[label])\n",
    "    return label_colors, label_color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def plot_heatmap(embs_df, emb_dims, label, output_file, kwargs_dict):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(15, 15), dpi=150)\n",
    "    label_colors, label_color_dict = make_colorbar(embs_df, label)\n",
    "\n",
    "    default_kwargs_dict = {\n",
    "        \"row_cluster\": True,\n",
    "        \"col_cluster\": True,\n",
    "        \"row_colors\": label_colors,\n",
    "        \"standard_scale\": 1,\n",
    "        \"linewidths\": 0,\n",
    "        \"xticklabels\": False,\n",
    "        \"yticklabels\": False,\n",
    "        \"figsize\": (15, 15),\n",
    "        \"center\": 0,\n",
    "        \"cmap\": \"magma\",\n",
    "    }\n",
    "\n",
    "    if kwargs_dict is not None:\n",
    "        default_kwargs_dict.update(kwargs_dict)\n",
    "    g = sns.clustermap(\n",
    "        embs_df.iloc[:, 0:emb_dims].apply(pd.to_numeric), **default_kwargs_dict\n",
    "    )\n",
    "\n",
    "    plt.setp(g.ax_row_colors.get_xmajorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    for label_color in list(label_color_dict.keys()):\n",
    "        g.ax_col_dendrogram.bar(\n",
    "            0, 0, color=label_color_dict[label_color], label=label_color, linewidth=0\n",
    "        )\n",
    "\n",
    "        g.ax_col_dendrogram.legend(\n",
    "            title=f\"{label}\",\n",
    "            loc=\"lower center\",\n",
    "            ncol=4,\n",
    "            bbox_to_anchor=(0.5, 1),\n",
    "            facecolor=\"white\",\n",
    "        )\n",
    "    \n",
    "    logger.info(f\"Output file: {output_file}\")\n",
    "    plt.savefig(output_file, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "\n",
    "def plot_umap(embs_df, emb_dims, label, output_file, kwargs_dict, seed=0):\n",
    "    only_embs_df = embs_df.iloc[:, :emb_dims]\n",
    "    only_embs_df.index = pd.RangeIndex(0, only_embs_df.shape[0], name=None).astype(str)\n",
    "    only_embs_df.columns = pd.RangeIndex(0, only_embs_df.shape[1], name=None).astype(\n",
    "        str\n",
    "    )\n",
    "    vars_dict = {\"embs\": only_embs_df.columns}\n",
    "    obs_dict = {\"cell_id\": list(only_embs_df.index), f\"{label}\": list(embs_df[label])}\n",
    "    adata = anndata.AnnData(X=only_embs_df, obs=obs_dict, var=vars_dict)\n",
    "    sc.tl.pca(adata, svd_solver=\"arpack\")\n",
    "    sc.pp.neighbors(adata, random_state=seed)\n",
    "    sc.tl.umap(adata, random_state=seed)\n",
    "    # sns.set(rc={\"figure.figsize\": (10, 10)}, font_scale=2.3)\n",
    "    sns.set(rc={\"figure.figsize\": (30, 30)}, font_scale=2.3)\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    default_kwargs_dict = {\"size\": 200}\n",
    "    if kwargs_dict is not None:\n",
    "        default_kwargs_dict.update(kwargs_dict)\n",
    "\n",
    "    cats = set(embs_df[label])\n",
    "\n",
    "    with plt.rc_context():\n",
    "        ax = sc.pl.umap(adata, color=label, show=False, **default_kwargs_dict)\n",
    "        # Move legend to bottom\n",
    "        ax.legend(\n",
    "            markerscale=2,\n",
    "            frameon=False,\n",
    "            loc=\"upper center\",                    # Changed from \"center left\"\n",
    "            bbox_to_anchor=(0.5, -0.05),          # Changed: (x, y) - x=0.5 centers horizontally, y=-0.05 places below plot\n",
    "            ncol=(3 if len(cats) <= 14 else 4 if len(cats) <= 30 else 5),  # Increased ncol for horizontal layout\n",
    "        )\n",
    "\n",
    "        plt.title(f\"Cell Type ({AGGREGATION_LEVEL})\", fontsize=16, pad=20)\n",
    "\n",
    "        \n",
    "        plt.savefig(output_file, bbox_inches=\"tight\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d35d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_embs(\n",
    "    embs,\n",
    "    plot_style,\n",
    "    output_directory,\n",
    "    output_prefix,\n",
    "    max_ncells,\n",
    "    max_ncells_to_plot=1000,\n",
    "    kwargs_dict=None,\n",
    "    emb_label=[\"cell_type\"],\n",
    "    labels_to_plot=[\"cell_type\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot embeddings, coloring by provided labels.\n",
    "\n",
    "    **Parameters:**\n",
    "\n",
    "    embs : pandas.core.frame.DataFrame\n",
    "        | Pandas dataframe containing embeddings output from extract_embs\n",
    "    plot_style : str\n",
    "        | Style of plot: \"heatmap\" or \"umap\"\n",
    "    output_directory : Path\n",
    "        | Path to directory where plots will be saved as pdf\n",
    "    output_prefix : str\n",
    "        | Prefix for output file\n",
    "    max_ncells_to_plot : None, int\n",
    "        | Maximum number of cells to plot.\n",
    "        | Default is 1000 cells randomly sampled from embeddings.\n",
    "        | If None, will plot embeddings from all cells.\n",
    "    kwargs_dict : dict\n",
    "        | Dictionary of kwargs to pass to plotting function.\n",
    "\n",
    "    **Examples:**\n",
    "\n",
    "    .. code-block :: python\n",
    "\n",
    "        >>> embex.plot_embs(embs=embs,\n",
    "        ...                 plot_style=\"heatmap\",\n",
    "        ...                 output_directory=\"path/to/output_directory\",\n",
    "        ...                 output_prefix=\"output_prefix\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if plot_style not in [\"heatmap\", \"umap\"]:\n",
    "        logger.error(\n",
    "            \"Invalid option for 'plot_style'. \" \"Valid options: {'heatmap','umap'}\"\n",
    "        )\n",
    "        raise\n",
    "\n",
    "    if (plot_style == \"umap\") and (labels_to_plot is None):\n",
    "        logger.error(\"Plotting UMAP requires 'labels_to_plot'. \")\n",
    "        raise\n",
    "\n",
    "    if max_ncells_to_plot is not None:\n",
    "        if max_ncells_to_plot > max_ncells:\n",
    "            max_ncells_to_plot = max_ncells\n",
    "            logger.warning(\n",
    "                \"max_ncells_to_plot must be <= max_ncells. \"\n",
    "                f\"Changing max_ncells_to_plot to {max_ncells}.\"\n",
    "            )\n",
    "        elif max_ncells_to_plot < max_ncells:\n",
    "            embs = embs.sample(max_ncells_to_plot, axis=0)\n",
    "\n",
    "    if emb_label is None:\n",
    "        label_len = 0\n",
    "    else:\n",
    "        label_len = len(emb_label)\n",
    "\n",
    "    emb_dims = embs.shape[1] - label_len\n",
    "\n",
    "    if emb_label is None:\n",
    "        emb_labels = None\n",
    "    else:\n",
    "        emb_labels = embs.columns[emb_dims:]\n",
    "\n",
    "    if plot_style == \"umap\":\n",
    "        for label in labels_to_plot:\n",
    "            if label not in emb_labels:\n",
    "                logger.warning(\n",
    "                    f\"Label {label} from labels_to_plot \"\n",
    "                    f\"not present in provided embeddings dataframe.\"\n",
    "                )\n",
    "                continue\n",
    "            output_prefix_label = output_prefix + f\"_umap_{label}\"\n",
    "            output_file = (\n",
    "                Path(output_directory) / output_prefix_label\n",
    "            ).with_suffix(\".pdf\")\n",
    "            plot_umap(embs, emb_dims, label, output_file, kwargs_dict)\n",
    "\n",
    "    if plot_style == \"heatmap\":\n",
    "        for label in labels_to_plot:\n",
    "            if label not in emb_labels:\n",
    "                logger.warning(\n",
    "                    f\"Label {label} from labels_to_plot \"\n",
    "                    f\"not present in provided embeddings dataframe.\"\n",
    "                )\n",
    "                continue\n",
    "            output_prefix_label = output_prefix + f\"_heatmap_{label}\"\n",
    "            output_file = (\n",
    "                Path(output_directory) / output_prefix_label\n",
    "            ).with_suffix(\".pdf\")\n",
    "            plot_heatmap(embs, emb_dims, label, output_file, kwargs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0777ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot UMAP of cell embeddings\n",
    "# note: scanpy umap necessarily saves figs to figures directory\n",
    "plot_embs(embs=embs, \n",
    "        plot_style=\"umap\",\n",
    "        max_ncells_to_plot= 100000, #10000 if DATASET == \"cellnexus_blood_disease\" else 1000,  # Set to None to plot all cells\n",
    "        output_directory=os.path.join(output_dir, \"embeddings\"),  \n",
    "        output_prefix=\"emb_plot\",\n",
    "        kwargs_dict=default_kwargs_dict,\n",
    "        max_ncells = 100000,\n",
    "        emb_label=[\"cell_type\"],\n",
    "        labels_to_plot=[\"cell_type\"],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4687e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap of cell embeddings\n",
    "# embex.plot_embs(embs=embs, \n",
    "#                 plot_style=\"heatmap\",\n",
    "#                 max_ncells_to_plot=10000 if DATASET == \"cellnexus_blood_disease\" else 1000,\n",
    "#                 output_directory=os.path.join(output_dir, \"embeddings\"),\n",
    "#                 output_prefix=\"heatmap_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0bccf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d0d09b9",
   "metadata": {},
   "source": [
    "# Check model frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a41f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model_path = \"/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_foundation_models/models/30M_AGGsinglecell_6_emb256_SL2048_E2_B12_LR0.001_LSlinear_WU10000_Oadamw/final_trained_model/singlecell/\"\n",
    "def_freeze_layers = 6 \n",
    "mode = \"train\"\n",
    "output_hidden_states = (mode == \"eval\")\n",
    "model_args = {\n",
    "        \"pretrained_model_name_or_path\": model_path,\n",
    "        \"output_hidden_states\": output_hidden_states,\n",
    "        \"output_attentions\": False,\n",
    "    }\n",
    "model_args[\"num_labels\"] = 9\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(**model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frozen_layers(model):\n",
    "    \"\"\"Count how many layers are completely frozen\"\"\"\n",
    "    frozen_layers = 0\n",
    "    total_layers = len(model.bert.encoder.layer)\n",
    "    \n",
    "    for i, layer in enumerate(model.bert.encoder.layer):\n",
    "        # Check if all parameters in this layer are frozen\n",
    "        layer_params = list(layer.parameters())\n",
    "        if layer_params:  # Make sure layer has parameters\n",
    "            all_frozen = all(not param.requires_grad for param in layer_params)\n",
    "            if all_frozen:\n",
    "                frozen_layers += 1\n",
    "            else:\n",
    "                break  # Assuming layers are frozen sequentially from start\n",
    "    \n",
    "    return frozen_layers, total_layers\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"Count total trainable vs frozen parameters\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, frozen, total\n",
    "\n",
    "# Before freezing\n",
    "print(\"=== BEFORE FREEZING ===\")\n",
    "frozen_layers_before, total_layers = count_frozen_layers(model)\n",
    "trainable_before, frozen_before, total_before = count_trainable_params(model)\n",
    "\n",
    "print(f\"Frozen layers: {frozen_layers_before}/{total_layers}\")\n",
    "print(f\"Trainable parameters: {trainable_before:,}\")\n",
    "print(f\"Frozen parameters: {frozen_before:,}\")\n",
    "print(f\"Total parameters: {total_before:,}\")\n",
    "\n",
    "# Your existing freezing code\n",
    "if def_freeze_layers > 0:\n",
    "    modules_to_freeze = model.bert.encoder.layer[:def_freeze_layers]\n",
    "    for module in modules_to_freeze:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# After freezing\n",
    "print(\"\\n=== AFTER FREEZING ===\")\n",
    "frozen_layers_after, total_layers = count_frozen_layers(model)\n",
    "trainable_after, frozen_after, total_after = count_trainable_params(model)\n",
    "\n",
    "print(f\"Frozen layers: {frozen_layers_after}/{total_layers}\")\n",
    "print(f\"Trainable parameters: {trainable_after:,}\")\n",
    "print(f\"Frozen parameters: {frozen_after:,}\")\n",
    "print(f\"Total parameters: {total_after:,}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Successfully froze {frozen_layers_after - frozen_layers_before} additional layers\")\n",
    "print(f\"Reduced trainable parameters by {trainable_before - trainable_after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trainable_components(model):\n",
    "    \"\"\"Analyze which components are still trainable\"\"\"\n",
    "    components = {\n",
    "        'embeddings': 0,\n",
    "        'encoder_layers': 0,\n",
    "        'pooler': 0,\n",
    "        'classifier': 0,\n",
    "        'other': 0\n",
    "    }\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param_count = param.numel()\n",
    "            \n",
    "            if 'embeddings' in name:\n",
    "                components['embeddings'] += param_count\n",
    "            elif 'encoder.layer' in name:\n",
    "                components['encoder_layers'] += param_count\n",
    "            elif 'pooler' in name:\n",
    "                components['pooler'] += param_count\n",
    "            elif 'classifier' in name:\n",
    "                components['classifier'] += param_count\n",
    "            else:\n",
    "                components['other'] += param_count\n",
    "                \n",
    "    return components\n",
    "\n",
    "# Analyze what's still trainable\n",
    "trainable_components = analyze_trainable_components(model)\n",
    "print(\"Trainable parameters breakdown:\")\n",
    "for component, count in trainable_components.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {component}: {count:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854f11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c8a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73287b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32cd5003",
   "metadata": {},
   "source": [
    "# Create Json of Dictionaries for IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065611a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "train_dataset_path = '/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_cell_classification_models/cell_type_classification/cellnexus_cell_types/30M_metacell_16/cellnexus_cell_types_test_labeled_train.dataset'\n",
    "test_dataset_path = '/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/trained_cell_classification_models/cell_type_classification/cellnexus_cell_types/30M_metacell_16/cellnexus_cell_types_test_labeled_test.dataset'\n",
    "output_dir = '/hpcfs/groups/phoenix-hpc-mangiola_laboratory/haroon/geneformer/datasets/cell_type_classification/cellnexus_cell_types/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "# Load the training dataset\n",
    "train_data = load_from_disk(train_dataset_path)\n",
    "print(f\"Train dataset loaded: {len(train_data)} samples\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = load_from_disk(test_dataset_path)\n",
    "print(f\"Test dataset loaded: {len(test_data)} samples\")\n",
    "\n",
    "# Set eval_size (you may need to adjust this value)\n",
    "eval_size = 0.1  # 20% for validation, adjust as needed\n",
    "\n",
    "print(f\"\\nPerforming train-validation split with eval_size={eval_size}...\")\n",
    "# Perform train-test split on the training data\n",
    "data_dict = train_data.train_test_split(\n",
    "    test_size=eval_size,\n",
    "    stratify_by_column=\"cell_type_backup\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Extract the split datasets\n",
    "train_split = data_dict['train']\n",
    "val_split = data_dict['test']  # This is actually validation data from the split\n",
    "\n",
    "print(f\"Train split: {len(train_split)} samples\")\n",
    "print(f\"Validation split: {len(val_split)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fed113",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split['sample_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bebd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split['sample_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract input_ids from each dataset\n",
    "print(\"\\nExtracting input_ids...\")\n",
    "train_ids = [str(sample) for sample in train_split['input_ids']]\n",
    "eval_ids = [str(sample) for sample in val_split['input_ids']]\n",
    "test_ids = [str(sample) for sample in test_data['input_ids']]\n",
    "\n",
    "print(f\"Train IDs: {len(train_ids)}\")\n",
    "print(f\"Eval IDs: {len(eval_ids)}\")\n",
    "print(f\"Test IDs: {len(test_ids)}\")\n",
    "\n",
    "# Check for overlaps\n",
    "print(\"\\nChecking for ID overlaps...\")\n",
    "train_set = set(train_ids)\n",
    "eval_set = set(eval_ids)\n",
    "test_set = set(test_ids)\n",
    "\n",
    "# Check overlaps between splits\n",
    "train_eval_overlap = train_set.intersection(eval_set)\n",
    "train_test_overlap = train_set.intersection(test_set)\n",
    "eval_test_overlap = eval_set.intersection(test_set)\n",
    "\n",
    "print(f\"Train-Eval overlap: {len(train_eval_overlap)} IDs\")\n",
    "print(f\"Train-Test overlap: {len(train_test_overlap)} IDs\")\n",
    "print(f\"Eval-Test overlap: {len(eval_test_overlap)} IDs\")\n",
    "\n",
    "if train_eval_overlap:\n",
    "    print(f\"WARNING: Found {len(train_eval_overlap)} overlapping IDs between train and eval!\")\n",
    "    print(f\"Sample overlapping IDs: {list(train_eval_overlap)[:5]}\")\n",
    "\n",
    "if train_test_overlap:\n",
    "    print(f\"WARNING: Found {len(train_test_overlap)} overlapping IDs between train and test!\")\n",
    "    print(f\"Sample overlapping IDs: {list(train_test_overlap)[:5]}\")\n",
    "\n",
    "if eval_test_overlap:\n",
    "    print(f\"WARNING: Found {len(eval_test_overlap)} overlapping IDs between eval and test!\")\n",
    "    print(f\"Sample overlapping IDs: {list(eval_test_overlap)[:5]}\")\n",
    "\n",
    "if not any([train_eval_overlap, train_test_overlap, eval_test_overlap]):\n",
    "    print(\"✓ No overlapping IDs found between splits!\")\n",
    "\n",
    "# Create the split dictionaries\n",
    "print(\"\\nCreating split dictionaries...\")\n",
    "train_test_id_split_dict = {\n",
    "    \"attr_key\": \"input_ids\",\n",
    "    \"train\": train_ids + eval_ids,  # Combine train and eval for training\n",
    "    \"test\": test_ids\n",
    "}\n",
    "\n",
    "train_valid_id_split_dict = {\n",
    "    \"attr_key\": \"input_ids\",\n",
    "    \"train\": train_ids,\n",
    "    \"eval\": eval_ids\n",
    "}\n",
    "\n",
    "# Save the dictionaries as JSON\n",
    "print(\"Saving split dictionaries as JSON...\")\n",
    "train_test_path = Path(output_dir) / \"train_test_id_split_dict.json\"\n",
    "train_valid_path = Path(output_dir) / \"train_valid_id_split_dict.json\"\n",
    "\n",
    "with open(train_test_path, 'w') as f:\n",
    "    json.dump(train_test_id_split_dict, f, indent=2)\n",
    "\n",
    "with open(train_valid_path, 'w') as f:\n",
    "    json.dump(train_valid_id_split_dict, f, indent=2)\n",
    "\n",
    "print(f\"✓ Train-test split dictionary saved to: {train_test_path}\")\n",
    "print(f\"✓ Train-validation split dictionary saved to: {train_valid_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples processed: {len(train_ids) + len(eval_ids) + len(test_ids)}\")\n",
    "print(f\"Train samples: {len(train_ids)}\")\n",
    "print(f\"Validation samples: {len(eval_ids)}\")\n",
    "print(f\"Test samples: {len(test_ids)}\")\n",
    "print(f\"Train+Val for final training: {len(train_ids + eval_ids)}\")\n",
    "print(\"\\nSplit dictionaries saved successfully!\")\n",
    "\n",
    "# Optional: Save a human-readable summary\n",
    "summary_path = Path(output_dir) / \"split_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f\"Dataset Split Summary\\n\")\n",
    "    f.write(f\"===================\\n\\n\")\n",
    "    f.write(f\"Source datasets:\\n\")\n",
    "    f.write(f\"- Train dataset: {train_dataset_path}\\n\")\n",
    "    f.write(f\"- Test dataset: {test_dataset_path}\\n\\n\")\n",
    "    f.write(f\"Split configuration:\\n\")\n",
    "    f.write(f\"- Eval size: {eval_size}\\n\")\n",
    "    f.write(f\"- Stratify by: cell_type_backup\\n\")\n",
    "    f.write(f\"- Random seed: 42\\n\\n\")\n",
    "    f.write(f\"Final splits:\\n\")\n",
    "    f.write(f\"- Train samples: {len(train_ids)}\\n\")\n",
    "    f.write(f\"- Validation samples: {len(eval_ids)}\\n\")\n",
    "    f.write(f\"- Test samples: {len(test_ids)}\\n\")\n",
    "    f.write(f\"- Total samples: {len(train_ids) + len(eval_ids) + len(test_ids)}\\n\\n\")\n",
    "    f.write(f\"ID Overlaps:\\n\")\n",
    "    f.write(f\"- Train-Eval: {len(train_eval_overlap)}\\n\")\n",
    "    f.write(f\"- Train-Test: {len(train_test_overlap)}\\n\")\n",
    "    f.write(f\"- Eval-Test: {len(eval_test_overlap)}\\n\")\n",
    "\n",
    "print(f\"✓ Summary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
